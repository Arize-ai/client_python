{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CIhm1oFbDIv"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
        "\n",
        "# Getting Started with the Arize Platform - Recommendation Systems\n",
        "\n",
        "**In this walkthrough, we are going to monitor and investigate performance of a recommendation system recently pushed into production.**\n",
        "\n",
        "In this walkthrough you will be taking on the persona of a machine learning engineer for a premium music service. After spending a great deal of time collecting customer data, training and testing various models your team has built a ML-powered recommendation engine to give your listeners personalized playlist recommendations based on their most listened to songs on their sound cloud. You are now ready to push your model into production and monitor how your recommendations are received by your user. With Arize you have the ability to monitor the performance of your model in production and compare it to your selected baseline. \n",
        "\n",
        "In this use case you will have the opportunity to:\n",
        "1. Explore the decisions made by your recommendation with Arize explainability features\n",
        "2. Monitor feature drift happening in real-time as a user's music preference changes \n",
        "3. Surface drift, data quality, data consistency issues to perform a root cause analysis \n",
        "4. Triage the impactful features that lead to drift \n",
        "5. Make proactive decisions to negate model degradation\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "**Dataset Generation**\n",
        "\n",
        "The [Top 5000 Albums of All Time](https://www.kaggle.com/michaelbryantds/top-5000-albums-of-all-time-rateyourmusiccom) dataset contains 5000 ranked albums including ranking, album name, artist name, release date, genres, descriptors, average rating, number of ratings, and number of reviews. The dataset was acquired via web scraping on 12 October 2021 and decided by vote from the users of [rateyourmusic.com](rateyourmusic.com). \n",
        "Although we will not be using this full dataset directly, we will be using the album name, artist name, release date, genres to generate user recommendations.\n",
        "\n",
        "Once your models are deployed into production, ML Observability provides deep understanding of your model‚Äôs performance, and root causing exactly why it‚Äôs behaving in certain ways. The Arize platform logs model inferences across training, validation and production environments without needing to store nor serve your models.  \n",
        "\n",
        "The inferences for this use case are the probabilities the user will play the recommended song. Once the recommendations are made for each user, the ground truth collected is an indicator as to whether or not the user listened to the song (or skipped the song) ‚Äîrepresented as 0 (skipped) or 1 (played). \n",
        "\n",
        "For example:\n",
        "\n",
        "Recommended Song  | Outcome     | Actual Score\n",
        "-----------------------|---------------------|------------------\n",
        "Livin' On A Prayer BY Bon Jovi                 | User skipped song   | 0\n",
        "Paradise City BY Guns N' Roses                   | User listened to song         | 1\n",
        "Creep (Explicit) BY Radiohead                   | User skipped song   | 0\n",
        "\n",
        "For a recommendation system will be using precision as our default performance metric to monitor model performance since precision is about retrieving the best items to the user (assuming that there are more useful items available).\n",
        "\n",
        "Precision is the number of selected items that are relevant. So suppose our recommender system selects 3 items to recommend to users out of which 2 are relevant then precision will be 66%. \n",
        "\n",
        "See equation: \n",
        "\n",
        "$$Precision = \\frac{TP}{TP+FP}$$\n",
        "\n",
        "TP = song recommended + song played\n",
        "\n",
        "FP = song recommended + song not played\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Recommendation Engines**\n",
        "\n",
        "Recommendation engines are more than a way of rearranging information available about user preferences and then using this information to provide informed recommendations on the basis of that information, it is a critical piece of thousands of core business models. If the recommendation systems in your company drift over time, user satisfaction, engagement will decrease, while churn and distrust in your company will increase. Having recommendations that are dynamic, adaptable and explainable are key to customer success and the success of a company's core business model. In this notebook we will utilize the user-song play count dataset to uncover different ways in which we can recommend new tracks to different users and then monitor the performance of our model in Arize.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHd2qdG1eS7R"
      },
      "source": [
        "# Step 0. Setup and Getting the Data\n",
        "The first step is to load pre-existing datasets for training, validation and prediction into the colab notebook from your data source. \n",
        "\n",
        "\n",
        "\n",
        "## Install Dependencies and Import Libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxGk-fOteUMK"
      },
      "outputs": [],
      "source": [
        "!pip install arize --upgrade -q\n",
        "\n",
        "import datetime\n",
        "import uuid\n",
        "from datetime import timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from arize.pandas.logger import Client, Schema\n",
        "from arize.utils.types import Environments, ModelTypes\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and libraries imported!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2-Db2IGUeDO"
      },
      "source": [
        "## **üåê Download the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA4xZbpmkX25"
      },
      "outputs": [],
      "source": [
        "# Preparing dataset for this tutorial\n",
        "valid_df = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/rec-sys-val.csv\"\n",
        ")\n",
        "prod_df = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/rec-sys-prod.csv\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Data successfully downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8bCCH9qfOWa"
      },
      "source": [
        "## Inspect the Data\n",
        "Take a quick look at the dataset. The data is a combination of song metadata (song title, genre, artist and year released) and user information (age, location, last genre played). The model performance is determined by precision of the user song predictions, if the precision is high then a majority of the songs recommended to the user were played by the user. \n",
        "\n",
        "**Note**: We also add timestamp according to `datetime.now()` so that it show up on your UI accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TD2Udm4-wT9"
      },
      "outputs": [],
      "source": [
        "prod_df.head(10)\n",
        "# We add timestamp according to datetime.now() so that it show up on your UI accordingly\n",
        "prod_df[\"prediction_ts\"] = prod_df[\"days_ago\"].apply(\n",
        "    lambda days: int((datetime.datetime.now() - timedelta(days=days)).timestamp())\n",
        ")\n",
        "prod_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3Ipx7f2ms47"
      },
      "source": [
        "# Step 1. Sending Data into Arize üí´\n",
        "\n",
        "## Import and Setup Arize Client\n",
        "\n",
        "First, copy the Arize `API_KEY` and `SPACE_KEY` from your admin page shown below!\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaaE8uG3nu1S"
      },
      "outputs": [],
      "source": [
        "SPACE_KEY = \"SPACE_KEY\"\n",
        "API_KEY = \"API_KEY\"\n",
        "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
        "\n",
        "# Saving model metadata for passing in later\n",
        "model_id = \"rec-sys-demo-model\"\n",
        "model_version = \"1.0\"\n",
        "\n",
        "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"‚ùå NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"‚úÖ Arize setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW1_Dz7tAGCG"
      },
      "source": [
        "## Using the Python SDK\n",
        "For our dataset, we have pre-formatted the feature names and dataframes for logging to Arize using our Python SDK through `arize.pandas.logger`. The `Schema` of your model specifies a mapping from column names for your logging DataFrame. \n",
        "\n",
        "Here's a summary below:\n",
        "\n",
        "| Schema Argument Name | Description |||\n",
        "|:- |:-|---|---|\n",
        "| `feature_column_names`| names of the columns representing features |||\n",
        "| `prediction_id_column_name`| list of unique ids you can use to use to match each record |||\n",
        "| `prediction_label_column_name`| predictions column name |||\n",
        "| `actual_label_column_name`| actuals column name |||\n",
        "| `timestamp_column_name`| timestamps for when predictions were made |||\n",
        "\n",
        "For more details on how to send data in production to Arize, check out some of our other logging tutorials and SDK documentations in Gitbook.\n",
        "\n",
        "[![Buttons_OpenOrange.png](https://storage.googleapis.com/arize-assets/fixtures/Buttons_OpenOrange.png)](https://docs.arize.com/arize/sdks-and-integrations/python-sdk/arize.pandas)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2wz4n7gUeDR"
      },
      "source": [
        "## Log Validation & Production Data to Arize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xh98htFxqcV"
      },
      "outputs": [],
      "source": [
        "# Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
        "feature_column_names = prod_df.columns[:18]\n",
        "\n",
        "validation_schema = Schema(\n",
        "    prediction_id_column_name=\"prediction_id\",\n",
        "    prediction_label_column_name=\"Recommended\",  # required by score_categorical\n",
        "    prediction_score_column_name=\"Confidence\",\n",
        "    actual_label_column_name=\"Played_or_skipped\",\n",
        "    actual_score_column_name=\"Actual_Confidence\",\n",
        "    feature_column_names=feature_column_names,\n",
        ")\n",
        "\n",
        "val_response = arize_client.log(\n",
        "    dataframe=valid_df,\n",
        "    model_id=model_id,\n",
        "    model_version=model_version,\n",
        "    batch_id=\"validation_test\",  # provide a batch_id to distinguish from other validation sets\n",
        "    model_type=ModelTypes.SCORE_CATEGORICAL,\n",
        "    environment=Environments.VALIDATION,\n",
        "    schema=validation_schema,\n",
        "    \n",
        ")\n",
        "\n",
        "production_schema = Schema(\n",
        "    prediction_id_column_name=\"prediction_id\",\n",
        "    prediction_label_column_name=\"Recommended\",  # required by score_categorical\n",
        "    prediction_score_column_name=\"Confidence\",\n",
        "    timestamp_column_name=\"prediction_ts\",\n",
        "    actual_label_column_name=\"Played_or_skipped\",\n",
        "    actual_score_column_name=\"Actual_Confidence\",\n",
        "    feature_column_names=feature_column_names,\n",
        ")\n",
        "\n",
        "prod_response = arize_client.log(\n",
        "    dataframe=prod_df,\n",
        "    model_id=model_id,\n",
        "    model_version=model_version,\n",
        "    model_type=ModelTypes.SCORE_CATEGORICAL,\n",
        "    environment=Environments.PRODUCTION,\n",
        "    schema=production_schema,\n",
        "    \n",
        ")\n",
        "\n",
        "# Checking responses to make sure our data was successfully ingested\n",
        "if val_response.status_code != 200:\n",
        "    print(\n",
        "        f\"logging failed with response code {val_response.status_code}, {val_response.text}\"\n",
        "    )\n",
        "elif prod_response.status_code != 200:\n",
        "    print(\n",
        "        f\"logging failed with response code {prod_response.status_code}, {prod_response.text}\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"‚úÖ You have successfully logged data to Arize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ2de8uUxcof"
      },
      "source": [
        "# Step 2. Confirm Data in Arize ‚úÖ\n",
        "\n",
        "Note that the Arize performs takes about 10 minutes to index the data.  While the model should appear immediately, the data will not show up until the indexing is complete. Feel free to head over to the **Data Ingestion** tab for your model to watch Arize works its magic!üîÆ\n",
        "\n",
        "**The next sections are screen captures for tutorials to setting-up the model we just sent in.**\n",
        "\n",
        "Feel free to follow and mirror our instructions to set-up the dashboards yourself, or simply read the guide below to see how Arize can quickly generate value for demand forecasting models.\n",
        "\n",
        "**‚ö†Ô∏è DON'T SKIP:**\n",
        "In order to move on to the next step, make sure your actuals and training/production sets are loaded into the platform. To check:\n",
        "1. Navigate to models from the left bar, locate and click on model **rec-sys-demo-model**\n",
        "2. On the **Overview Tab**, make sure you can see Predictions and Actuals under the **Model Health** section. Once production actuals have been fully recorded on Arize, the row title will change from **0 Actuals** to **Actuals** with summary statistics such as cardinality listed in the tables.\n",
        "3. Verify the list of **Features** below **Actuals**.\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/predictionvolume.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3STlYp9Kd5Kr"
      },
      "source": [
        "# Step 3. Configure Baseline and Monitors\n",
        "Arize can automatically configure monitors that are best suited to your data. From the banner at the top of the screen, simply click **Set up Model** then select **Validation Version 1.0** and click **NEXT**. Select **Precision** as the **Default Metric**.\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Click-Through%20Rate%20Use-Case/images/initial_setup_banner.png)\n",
        "\n",
        "You will now see that the baseline has been set and **Drift**, **Data Quality**, and **Performance** monitors have been created!!!\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/monitors.png\" width=\"300\"/>\n",
        "\n",
        "## Check Triggered Monitors \n",
        "\n",
        "You are now able to check on and edit pre-configured, i.e **managed**, monitors on the **monitor** tab. \n",
        "\n",
        "\n",
        "To automatically be notified of poor values getting introduced in production create custom monitors for your features. From the model [**Monitors**](https://docs.arize.com/arize/product-guides/monitors) tab click **New Monitor** and choose a monitor type (drift, performance, or data quality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn6-4AkF4DKx"
      },
      "source": [
        "# Step 4. First glance\n",
        "Now that your baseline and monitors are configured, take a quick glance at the **Drift** tab. From the top right of the screen, pick **-30 Days** as the Date range. You may notice a slight dips in Precision at (9/29 for example in this screen shot). We will investigate this change in later sections.\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/drift-tab.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqfL0vIYHbf7"
      },
      "source": [
        "# Step 5. Analyzing feature drift\n",
        "During initial model setup, Arize automatically created managed monitors for monitoring drift, performance, and data quality across feature inputs. As we scroll through the features in the drift tab we see several triggered monitors. Let's check out the feature corresponding to the drift alert, in this case you should notice the monitor triggered for the **Affiliate Provider** feature. \n",
        "\n",
        "You can nagivate to the **Affiliate Provider** feature from either the model overview page or the **drift** tab --> **feature drift**, then click the feature to notice the change in the distribution. Use the PSI (population stability index) graph to select a period of interest. We see that not has the expected amount of **direct** links has decreased in production, as well as a new input has been seen by the model. Notice that as the new affiliate provider **facebook** appears, the default threshold set by Arize was crossed, this is due to the feature drift being caused by inputs not used in the training baseline. At this point it might be a good time to train your model on the new input. \n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/affiliate_drift.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP7QwjboHbpc"
      },
      "source": [
        "\n",
        "# Step 6. Analyzing and Detect Missing Data\n",
        "In the production data, values like **facebook** are being recorded against the **affiliate_provider** feature that were not part of the training data. As we scroll down we see that a monitor for data quality has been triggered on the **first_affiliate_tracked** feature due to the input data exceeding the alert threshold for percent of empty values. We can further investigate by navigating to **% Empty** , clicking on it and then being directed to the **Data Quality** tab. Here you see the dramatic increase in missing values over time.\n",
        "\n",
        "Make sure to view the last 30 days by selecting the correct range in the top right corner of the screen. Arize keeps tracks of feature cardinality as well as fields with no data and can pin-point the exact time that this issue started.\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/rec-sys-data-quality.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmgQACSLHb05"
      },
      "source": [
        "# Step 7. Analyzing model performance\n",
        "\n",
        "Now that we have looked over data quality and feature drift, we will investigate model performance with Arize's performance dashboards. While you are able to fully customize dashboards for your team, we will use a **scored model** template from the **template** tab in the left-hand sidebar. \n",
        "\n",
        "[Dashboards](https://docs.arize.com/arize/product-guides/dashboards) are comprised of widgets designed for different types of analysis across your training, validation, and production environments:\n",
        "- Distribution Widget for analyzing data distribution changes over Feature, Prediction, and Actuals.\n",
        "- TimeSeries Widget for analyzing time-based data. \n",
        "- Statistic Widget for getting an aggregate statistic. Data Metrics and Evaluation Metrics charts are also available for this widget. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/Screen%20Shot%202021-10-19%20at%207.10.27%20PM.png)\n",
        "\n",
        "An alternative way to create this is by navigating through **Dashboards Tab**\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/rec-sys-create-dashboard.gif)\n",
        "\n",
        "Once we have the dashboard loaded the [Performance Dashboard](https://docs.arize.com/arize/product-guides/performance) provides you with aggregate statistics for accuracy, recall, and other customizable evaluation and data metrics. Dashboards facilitate performance troubleshooting by providing support for customizable widgets and chainable filters to drill down to specific cohorts and see respective model statistics. You can slice and filter Dashboards by any model, model version, feature, and/or actual value.\n",
        "\n",
        "\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/Screen%20Shot%202021-10-19%20at%2010.03.38%20PM.png)\n",
        "\n",
        "You can also slice and filter dashboards by any model, model version, and model dimension. As you can see when we filter on major cities like LA, NYC and Austin we see the precision drops from **0.702** (top screen, right most widget) to **0.54** (bottom screen, right most widget). It appears something is going on at the cohort level which is causing performance to degrade, we will investigate further. \n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/Screen%20Shot%202021-10-19%20at%2010.08.54%20PM.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etxzt4IIIRRA"
      },
      "source": [
        "# Step 8. Heatmap view\n",
        "\n",
        "When using the performance dashboards we have a 2D view of the data, when we use our Heatmap view we can take the performance analysis a step further with a 3D view. We can also create a **Feature Heatmap Dashboard** from our **Dashboards Tab**\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/rec-sys-data-create-heatmap.gif)\n",
        "\n",
        "The [Feature Performance Heatmap](https://docs.arize.com/arize/product-guides/performance) provides you with model performance information across all features, at various feature/value combinations ‚Äî also known as a slice. Feature Performance Heatmaps also support conditional filters (like Dashboards).\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/Screen%20Shot%202021-10-19%20at%2010.11.26%20PM.png)\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/windows.png)\n",
        "Once you have an idea of your overall model performance across various slices it's time to start diving into which features could be causing this performance degradation. At first glance we see that while most city inputs to the feature **city** are performing quite well, the input **Austin** is performing poorly and contributes a large impact on performance.\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/Austin.png)\n",
        "When we scroll down to When we decided to filter on **genre_last_played** we see a sharp spike in **Heavy Metal, Hard Rock**, this could be a bug in the data pipeline causing data quality issues, or the model being deployed in a new domain, or an outlier event (like a large music festival in one of the input cities). At the point you can save the production data and retrain your model to increase performance on this specific feature.\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/metal.png)\n",
        "\n",
        "Heatmaps also rank order the the worst performing slices to automatically surface potential root causes of your performance degradation. To access these slices click on the fire icon on the right (it‚Äôs a white outline icon) when you‚Äôre not in edit mode (the screenshot is in edit mode).\n",
        "\n",
        "\n",
        "\n",
        "This view has automatically identified that a good first step in improving overall model performance begins with improving the model performance for the slices where feature **genre_last_played** = **Heavy Metal, Hard Rock** and **city** = **Austin**. \n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/slices.png)\n",
        "\n",
        "Now if we filter down on both **genre_last_played** = **Heavy Metal, Hard Rock** and **city** = **Austin** we find that with a precision of 0.035 there were very few songs recommended to subscribers in Austin who had heavy metal as their last song played received a song prediction through the platform which they played (they skipped it). Now either the data ingestion needs to be fixed or your model needs to be retrained to handle Austin's new Heavy Metal scene! \n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Rec-Sys-Use-Case/featuresheatmap.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV7Ke5xq7pM6"
      },
      "source": [
        "\n",
        "# üìö Conclusion\n",
        "\n",
        "In this walkthrough we've shown how Arize can be used to log prediction data for a model, pinpoint model performance degradation, and set up monitors to catch future issues. We have been able to identify 3 areas of concern:\n",
        "\n",
        "1. Surface drift, data quality, data consistency issues to perform a root cause analysis as a new domain appeared in our production system for which the model was not trained on. \n",
        "2. Monitor feature drift happening in real-time as a user's music preference changes and find new inputs to features which appeared in production.\n",
        "3. We use performance dashboard and heatmap views to isolate abnormal inputs to features in production.\n",
        "\n",
        "\n",
        "\n",
        "Though we covered a lot of ground, this is just scratching the surface of what the Arize platform can do. We urge you to explore more of Arize, either on your own or through one of our many other tutorials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlEOkXo-GjMS"
      },
      "source": [
        "### About Arize\n",
        "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
        "- Automated ML monitoring and model monitoring\n",
        "- Workflows to troubleshoot model performance\n",
        "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
        "- Model prediction cohort analysis\n",
        "- Pre-deployment model validation\n",
        "- Integrated model explainability\n",
        "\n",
        "### Website\n",
        "Visit Us At: https://arize.com/model-monitoring/\n",
        "\n",
        "### Additional Resources\n",
        "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
        "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
        "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
        "- [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/)\n",
        "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
        "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
        "- [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/)\n",
        "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
        "\n",
        "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Recommendation_system_usecase.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ec6620dbdb1ae7ae605320a7ce7472136ac4f41aa18fe452eefaab7683971943"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit (conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
