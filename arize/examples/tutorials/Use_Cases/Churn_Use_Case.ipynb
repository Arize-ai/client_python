{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-Y39M6oFP3"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
        "\n",
        "# Getting Started with the Arize Platform - Churn Prediction in the Telecom Industry\n",
        "\n",
        "\n",
        "At a large telecom organization, you are responsible for managing the model that predicts which customers will churn so marketing efforts can be targeted to save their accounts!\n",
        "\n",
        "In this walkthrough, we are going to investigate your churn model in production. We will validate degradation in model performance, troubleshoot the root cause, and furthermore set up proactive monitors to mitigate the impact of future degradations.\n",
        "\n",
        "We will set up monitors to practively identify when our churn model is not perfoming as expected, troubleshoot why we're seeing this deviation in production, and come up with actionable steps to improve the model.\n",
        "\n",
        "Our steps to resolving this issue will be:\n",
        "\n",
        "Get our model onto the Arize platform to investigate\n",
        "\n",
        "1. Setup a performance dashboard to look at prediction performance\n",
        "2. Review a recent instance of model drift\n",
        "3. Understand where the model is underperforming\n",
        "4. Discover the root cause of why a slice (grouping) of predictions are underperforming\n",
        "\n",
        "The production data contains 1 month of data where 2 main issues exist. You will work on identifying these issues over the course of this exercise.\n",
        "\n",
        "1. Customers `SupportElgiblity` expired \n",
        "2. Identify what type of customers is the model poor at prediction to churn or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxW8z0PRpO6K"
      },
      "source": [
        "# Step 0. Setup and Getting the Data\n",
        "\n",
        "The first step is to load our preexisting dataset which includes training and production environments for our churn predicting model. Using a preexisting dataset illustrates how simple it is to get started with the Arize platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGVYzqZXpSNF"
      },
      "source": [
        "## Install Dependencies and Import Libraries üìö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOrR6z5y4C14",
        "outputId": "8a4a9989-9575-4a50-ab59-45ccc1833242"
      },
      "outputs": [],
      "source": [
        "!pip install arize --upgrade -q\n",
        "\n",
        "import datetime\n",
        "import uuid\n",
        "from datetime import timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from arize.pandas.logger import Client, Schema\n",
        "from arize.utils.types import Environments, ModelTypes\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and libraries imported!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N3_Iiy3pVRM"
      },
      "source": [
        "## üåê **Download the Data**\n",
        "In this walkthrough, we‚Äôll be sending real historical data (with privacy conscious changes to feature names and values). Note, that while feature names and values are made explicit in this dataset, you can achieve the same level of ML Observability using obfuscated features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08yiYCFyqEut"
      },
      "source": [
        "For this historical evaluation case, the best approach to send data into Arize is via the [Python SDK pandas logger](https://docs.arize.com/arize/api-reference/python-sdk/arize.pandas). Therefore, you will need to have a Pandas dataframe for each dataset/environment. \n",
        "\n",
        "We have **2 Environments: training and production**. Training and production are two different datasets that correspond to their respective parts of the training/production pipeline. We download each of them, storing them in a dictionary `datasets` for later use. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61nHOpISqGjX",
        "outputId": "037d7dea-6ceb-47a0-c726-3ab163892cc8"
      },
      "outputs": [],
      "source": [
        "datasets = {}\n",
        "environments = [\"production\", \"training\"]\n",
        "for environment in environments:\n",
        "\n",
        "    filepath = (\n",
        "        \"https://storage.googleapis.com/arize-assets/fixtures/Tags-Demo-Data/churn_prediction_\"\n",
        "        + environment\n",
        "        + \".csv\"\n",
        "    )\n",
        "\n",
        "    # Create the dataframe and store in dictionary\n",
        "    datasets[environment] = pd.read_csv(filepath)\n",
        "\n",
        "print(\"‚úÖ Data successfully downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMTYEbBDn-e8"
      },
      "source": [
        "## Features Description \n",
        "**Tenure** - How long the customer has been with our organization\n",
        "\n",
        "**PhoneService** - Does this customer have a phone plan?\n",
        "\n",
        "**MultipleLines**\t- Does this customer have multiple lines?\n",
        "\n",
        "**Internet_Speed** - How fast is the customer Internet speed?\n",
        "\n",
        "**SupportElgilblity** - How can this customer receive service for troubleshooting?\n",
        "\n",
        "**Contract** - What sort of contract terms does this customer have?\n",
        "\n",
        "**MonthlyStreamingTime** - How much does the customer stream per month?\n",
        "\n",
        "**PremiumHDStreaming** - Does this customer stream HD content?\n",
        "\t\n",
        "**Prediction_Label:** predicted churn or not churn\n",
        "\n",
        "**PREDICTION_SCORE:** predicted probability of churn\n",
        "\n",
        "**ACTUAL:** actual value, churn or not_churn\n",
        "\n",
        "**ACTUAL_SCORE:** 1.0 if churn, 0.0 if not_churn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4tIvDR7qKKY"
      },
      "source": [
        "## Inspect the Data \n",
        "\n",
        "Take a quick look at the dataset. The data represents a model designed and trained to evaluate the probability of a customer churning based on various features such as contract, internet speed and techsupport, etc. The dataset contains about one month of data and the performance will be evaluated by comparing:\n",
        "\n",
        "*   **PREDICTION**: The probability of a customer will churn predicted by the model \n",
        "*   **ACTUAL**: Churn or not churn based on ground truth collected by your company"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "xyayJrXBqNBg",
        "outputId": "010559db-c203-49ef-bb40-8785835d99db"
      },
      "outputs": [],
      "source": [
        "datasets[\"production\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMv_yustAj3_"
      },
      "outputs": [],
      "source": [
        "features_column_names = [\n",
        "    \"Tenure\",\n",
        "    \"PhoneService\",\n",
        "    \"MultipleLines\",\n",
        "    \"InternetSpeed\",\n",
        "    \"SupportEligibility\",\n",
        "    \"Contract\",\n",
        "    \"MonthlyStreamingTime\",\n",
        "    \"PremiumHDStreaming\",\n",
        "]\n",
        "shap_column_names = [f\"{x}_shap\" for x in features_column_names]\n",
        "tag_column_names = ['Gender', 'Age', 'Dependents', 'Partner', 'EmploymentStatus', 'LocationCode','Education']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ozBfSuQqPTy"
      },
      "source": [
        "# Step 1. Sending Data into Arize üí´\n",
        "\n",
        "Now that we have our dataset imported, we are ready to integrate into Arize. We do this by logging (sending) important data we want to analyze to the platform. There, the data will be easily visualized and troubleshooting workflows will help us find the source of our problem.\n",
        "\n",
        "For our model, we are going to log:\n",
        "*   feature data\n",
        "*   predictions\n",
        "*   actuals\n",
        "\n",
        "## Import and Setup Arize Client\n",
        "\n",
        "The first step is to setup our Arize client. After that we will log the data.\n",
        "\n",
        "First, use your Arize account credentials to log in. Thereafter, retrieve the Arize `API_KEY` and `SPACE_KEY` from your admin page shown below! Copy those over to the set-up section. We will also be setting up some metadata to use across all logging.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-YTIj6WqRGn",
        "outputId": "b661f808-0f08-48ed-c1e3-5aa3ede1f5b3"
      },
      "outputs": [],
      "source": [
        "SPACE_KEY = \"SPACE_KEY\"\n",
        "API_KEY = \"API_KEY\"\n",
        "\n",
        "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
        "\n",
        "model_id = (\n",
        "    \"churn-prediction-demo-model\"  # This is the model name that will show up in Arize\n",
        ")\n",
        "\n",
        "model_version = \"v1.0\"  # Version of model - can be any string\n",
        "\n",
        "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"‚ùå NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"‚úÖ Arize setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzcDC2jhqXFd"
      },
      "source": [
        "## Log Training Data\n",
        "\n",
        "Now that our Arize client is setup, let's go ahead and log all of our data to the platform. For more details on how **`arize.pandas.logger`** works, visit out documentations page below.\n",
        "\n",
        "[![Buttons_OpenOrange.png](https://storage.googleapis.com/arize-assets/fixtures/Buttons_OpenOrange.png)](https://docs.arize.com/arize/sdks-and-integrations/python-sdk/arize.pandas)\n",
        "\n",
        "Key parameters:\n",
        "\n",
        "*   **prediction_label_column_name**: tells Arize which column contains the predictions\n",
        "*   **actual_label_column_name**: tells Arize which column contains the actual results from field data\n",
        "*   **preidction_score_column_name**: tells Arize which column contains the prediction score from field data\n",
        "*   **actual_label_column_name**: tells Arize which column contains the actual results from field data\n",
        "*   **actual_score_column_name**: tells Arize which column contains the actual score from field data\n",
        "\n",
        "Given that our model is predicting between categories, we will use [ModelTypes.SCORE_CATEGORICAL](https://docs.arize.com/arize/product-guides-1/models/model-types) to perform this analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1C-RN8oqYr0"
      },
      "outputs": [],
      "source": [
        "df_training = datasets[\"training\"]\n",
        "\n",
        "\n",
        "def simulate_timestamps(X, days=30, add_days=0):\n",
        "    t = datetime.datetime.now() + timedelta(days=add_days)\n",
        "    current_ts, earlier_ts = t.timestamp(), (t - timedelta(days=days)).timestamp()\n",
        "    return pd.Series(np.linspace(earlier_ts, current_ts, num=len(X)), index=X.index)\n",
        "\n",
        "\n",
        "df_training[\"prediction_ts\"] = simulate_timestamps(df_training)\n",
        "df_training[\"prediction_id\"] = [str(uuid.uuid4()) for _ in range(df_training.shape[0])]\n",
        "\n",
        "\n",
        "# Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
        "training_schema = Schema(\n",
        "    prediction_id_column_name=\"prediction_id\",\n",
        "    timestamp_column_name=\"prediction_ts\",\n",
        "    prediction_label_column_name=\"Prediction_Labels\",\n",
        "    prediction_score_column_name=\"Prediction_Scores\",\n",
        "    actual_label_column_name=\"Actual_Labels\",\n",
        "    actual_score_column_name=\"Actual_Scores\",\n",
        "    feature_column_names=features_column_names,\n",
        "    tag_column_names = tag_column_names,\n",
        ")\n",
        "\n",
        "# Logging Training DataFrame\n",
        "training_response = arize_client.log(\n",
        "    dataframe=df_training,\n",
        "    model_id=model_id,\n",
        "    model_version=\"v2.0\",\n",
        "    model_type=ModelTypes.SCORE_CATEGORICAL,\n",
        "    environment=Environments.TRAINING,\n",
        "    schema=training_schema,\n",
        ")\n",
        "\n",
        "# If successful, the server will return a status_code of 200\n",
        "if training_response.status_code != 200:\n",
        "    print(\n",
        "        f\"logging failed with response code {training_response.status_code}, {training_response.text}\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"‚úÖ You have successfully logged training set to Arize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61FCDHEuQc2m"
      },
      "source": [
        "## Log Production Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da_6nZFCqaOv",
        "outputId": "ddbf2645-49b2-40b1-8ebd-75df9832d262"
      },
      "outputs": [],
      "source": [
        "df_production = datasets[\"production\"]\n",
        "\n",
        "\n",
        "def simulate_timestamps(X, days=30):\n",
        "    t = datetime.datetime.now()\n",
        "    current_ts, earlier_ts = t.timestamp(), (t - timedelta(days=days)).timestamp()\n",
        "    return pd.Series(np.linspace(earlier_ts, current_ts, num=len(X)), index=X.index)\n",
        "\n",
        "\n",
        "df_production[\"prediction_ts\"] = simulate_timestamps(df_production)\n",
        "df_production[\"prediction_id\"] = [\n",
        "    str(uuid.uuid4()) for _ in range(df_production.shape[0])\n",
        "]\n",
        "\n",
        "production_schema = Schema(\n",
        "    prediction_id_column_name=\"prediction_id\",\n",
        "    timestamp_column_name=\"prediction_ts\",\n",
        "    prediction_label_column_name=\"Prediction_Labels\",\n",
        "    prediction_score_column_name=\"Prediction_Scores\",\n",
        "    actual_label_column_name=\"Actual_Labels\",\n",
        "    actual_score_column_name=\"Actual_Scores\",\n",
        "    feature_column_names=features_column_names,\n",
        "    shap_values_column_names=dict(zip(features_column_names, shap_column_names)),\n",
        "    tag_column_names = tag_column_names,\n",
        ")\n",
        "\n",
        "production_response = arize_client.log(\n",
        "    dataframe=df_production,\n",
        "    model_id=model_id,\n",
        "    model_version=model_version,\n",
        "    model_type=ModelTypes.SCORE_CATEGORICAL,\n",
        "    environment=Environments.PRODUCTION,\n",
        "    schema=production_schema,\n",
        ")\n",
        "\n",
        "if production_response.status_code != 200:\n",
        "    print(\n",
        "        f\"logging failed with response code {production_response.status_code}, {production_response.text}\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"‚úÖ You have successfully logged production set to Arize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYlYF_Y4qbok"
      },
      "source": [
        "# Step 2. Confirm Data in Arize ‚úÖ\n",
        "\n",
        "Note that the Arize performs takes about 10 minutes to index the data. While the model should appear immediately, the data will not show up until the indexing is complete. Feel free to head over to the **Data Ingestion** tab for your model to watch Arize works its magic!üîÆ\n",
        "\n",
        "**‚ö†Ô∏è DON'T SKIP:**\n",
        "In order to move on to the next step, make sure your actuals and training/production sets are loaded into the platform. To check:\n",
        "1. Navigate to models from the left bar, locate and click on model **arize-demo-churn-prediction** or what you used as the `model-id`\n",
        "2. On the **Overview Tab**, make sure you can see Predictions and Actuals under the **Model Health** section. Once production actuals have been fully recorded on Arize, the row title will change from **0 Actuals** to **Actuals** with summary statistics such as cardinality listed in the tables.\n",
        "3. Verify the list of **Features** below **Actuals**.\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-confirmingest.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00UlbATTqeTL"
      },
      "source": [
        "# Step 3. Set up Model Baseline & Managed Monitors\n",
        "\n",
        "Now that our data has been logged into the [Arize platform](https://app.arize.com/) we can begin our investigation into our poorly performing fraud detection model. \n",
        "\n",
        "Arize will guide you through setting up a **Baseline** (reference environment for comparison) and automatically create **Monitors** for your model in just a few clicks ‚Äîjust follow the blue banner at the top of the page titled \"Finish setting up your model\". \n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Click-Through%20Rate%20Use-Case/images/initial_setup_banner.png)\n",
        "\n",
        "Arize can automatically configure monitors that are best suited to your data. From the banner at the top of the screen, select the following configurations after clicking the 'Set up Model' button: \n",
        "\n",
        "1. Datasets: `Training Version 1.0`\n",
        "2. Default Metric: `Accuracy`, Trigger Alert When: `Accuracy is below .8`, Positive Class: `churn`\n",
        "3. Turn On Monitoring: Drift ‚úÖ, Data Quality ‚úÖ, Performance ‚úÖ \n",
        "\n",
        "You will now see that the baseline has been set and **Drift**, **Data Quality**, and **Performance** monitors have been created!!! \n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-initalsetup.gif)\n",
        "\n",
        "These monitors will help ensure your team to proactively address performance, drift or data quality spikes before the issue grows into a larger issue. Monitors are able to be filtered by category, edit evaluation windows, thresholds, etc. and create custom monitors by visiting the **Monitors** tab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgXLESJeoqwU"
      },
      "source": [
        "# Step 4. Setting up a Custom Monitor\n",
        "\n",
        "During the initial setup process that we just went through, Arize automatically created global monitors to track performance, drift and data quality but every model needs custom monitors to ensure that all relevant performance is tracked. \n",
        "\n",
        "In addition to accuracy, it is important to track other performance metrics like \"False Negative Rate\" which indicates how many accounts churned without any effort to save them. \n",
        "\n",
        "The gif below outlines the process to create a monitor utilizing that metric. There is much more configurations that you can apply so try out applying a few filters, changing the evaluation window or even the metric type itself!\n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-setupmonitor.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_6-P9lRqglz"
      },
      "source": [
        "# Step 5. Exploring Drift \n",
        "\n",
        "Navigating to the **Drift** tab where model drift is visualized across the selected time period. You will see that the model experienced a spike in drift a few weeks ago so let's look into it!\n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-drifttab.gif)\n",
        "\n",
        "You can select a period in time on the graph and you will see the prediction distributions adjust accordingly below the graph. Predicted \"no_churn\" has increased compared to our baseline as well as there's much less predicted churn. \n",
        "\n",
        "Scroll down further in the page and you will find features of the model listed in order of their prediction drift impact score; the higher score means it contributed more to the model drift. This score is a weighted system that Arize calculates using SHAP values and the feature drift so that you don't need to serach for the top features that are causing drift.\n",
        "\n",
        "As you select the top feature, Support Elgiblity, there is a familiar page that visualizes drift of the feature with the distribution of the inputs to the feature below it. If you select a period of time where there was high drift, you will see that there was a new input discovered which is expired. \n",
        "\n",
        "This means that many customer's support elgiblity expired and the model was not trained with this input leading to the model to drift. \n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-driftnewinput.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1OSn3PG_53e"
      },
      "source": [
        "# Step 6. Exploring Performance Degradation\n",
        "\n",
        "Navigate to the **Performance** tab within Arize, you will see that accuracy (our default performance metric) is plotted over the last 30 days and it is overlaid on top of bars which measure the volume of predictions. Our model is doing pretty well but there have been a few dips in accuracy down to ~60% so let's look into what could be driving performance down.\n",
        "\n",
        "If you scroll down, the **Output Segmentation** section includes a confusion matrix which is useful for our model as it is assigning a class to the prediction.\n",
        "\n",
        "Let's scroll down even further to the **Performance Breakdown by Feature**, this section is very useful to uncover low performing cohorts within a feature. Since this model is producing SHAP values for every prediction, Arize is able to use those SHAP values to weight performance within each feature to create a **Performance Impact Score**. By sorting by this score instead of just feature importance or min/max performance, Arize is able to surface to the top, the top features that are attributing to decreased performance.\n",
        "\n",
        "At the top of the **Performance Breakdown by Feature** list is \"MonthlyStreamingTime\" so let's expand this feature. Now we see a list of the inputs to this feature which are a couple of categorical values. By hovering over bar, Arize displays the volume that this input was utilized in predictions and the performance metric for that cohort. \n",
        "\n",
        "You can see that 100hrs+ stands out a bit with accuracy at ~72% which is much lower than our global accuracy metric of ~88%. Click on the bar and then select \"add cohort as filter\". This now applies a conditional filter across the entire data set.\n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-streamingtimeperf.png)\n",
        "\n",
        "Now you will see that InternetSpeed is the top of the features and another low performing segment exposes itself which is \"Low Speed\" so let's add that as another filter.\n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-internetspeedperf.png)\n",
        "\n",
        "Once that filter is applied, you will see that the histograms across the board update from yellow/white which indicates high accuracy to darker red hues which indicates low accuracy. \n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-finalperf.png)\n",
        "\n",
        "In summary, we decomposed our model's global accuracy metric of ~88% and identified a segment of our users that our model is worse than a coin flip of predicting churn which is customers who stream over 100 hours and have slow internet speeds! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RilKqNwnkBf"
      },
      "source": [
        "# Step 7. Dashboarding Overview\n",
        "\n",
        "As we continue to check in and improve our model's performance, we want to be able to quickly and efficiently view all our important model metrics in a single pane. In the same way we set up a **Feature Performance Heatmap** we will create a **Model Performance Dashboard** to view our model's most important metrics in a single configurable layout. \n",
        "\n",
        "Navigate to the **Templates** section on the left sidebar and scroll down to click on the **Scored Model**. From there select your model, features you care to investigate, and positive class `Churn`. \n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-creatingdashboard.gif)\n",
        "\n",
        "In addition to the default widgets Arize sets up for your dashboard, you can configure custom metrics your team cares about. In only a few clicks I added a few widgets to give me a single glance view of my model's **Accuracy**, **False Positive Rate**, and **False Negative Rate** as standalone statistics widgets. To visualize these metrics over time I also created a timeseries widget and overlayed three plots showcase the fluctuation of my metrics over time. \n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-customdashboard.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlc32ATEbVy0"
      },
      "source": [
        "# Step 8. Business Impact\n",
        "Complimentary to traditional statistical measures to define model performance, business impact is another way to measure performance and obtain insights. By using Arize's Business Impact tool, you are able to experiement with different thresholds aka the decision boundary for a scored model. The Arize platform allows you to enter custom formulas, mapping model performance to your definition of model performance. Navigate to the Business Impact tab to set up a custom formula used to calculate the business impact of our model's performance to the overall profit/loss of your company. \n",
        "\n",
        "For example, the diagram below estimates the profit/loss of a decision made by our model.\n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-payoffdiagram.png)\n",
        "\n",
        "\n",
        "Capturing this in our **Business Impact** tab visualizes the profit/loss based on our model's prediction threshold for churn classification. ![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-payoffcurve.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvzm9Pn0bXE1"
      },
      "source": [
        "# Step 9. Explainability\n",
        "\n",
        "This churn model sent SHAP values for each prediction to Arize and earlier within this notebook, we saw how Arize can utilize feature importance across the platform but you are also able to explore explainability by itself within our Explainability tab. \n",
        "\n",
        "Within this page, you are able to view the gloal feature imporance of all your predictions as well as apply cohort prediction based analysis to obtain insights. \n",
        "\n",
        "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/churn/visuals/churn-explainability.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rc2y7IEd2hk"
      },
      "source": [
        "# üìö Conclusion \n",
        "In this walkthrough we've shown how Arize can be used to log prediction data for a model, pinpoint model performance degradations, set up monitors to proactively catch future issues, create dashboards for at a glance model understanding, and calculate business impact metrics through custom formulas. \n",
        "\n",
        "We completed the following tasks: \n",
        "1. Uploaded data from a model predicting who will churn \n",
        "2. Set up a model baseline (Training V1.0) and managed Performance, Data Quality, and Drift monitors\n",
        "3. Uncovered low performing segments of our population within the Performance tab of customers who have high `monthlystreamingtime` and slow `InternetSpeed`.\n",
        "4. Identified correlations between our model's degrading performance with individual feature drift and distribution variance. We found that there were many customers who had their `SupportElgibilty` expired which was a new input that caused the model to drift. It could make sense to train the model on this input. \n",
        "5. Created a model Performance Dashboard to visualize important metrics at a glance with custom timeseries metrics for ongoing analysis \n",
        "6. Captured our potential profit/loss based on our model's classification threshold using the Business Impact tab\n",
        "7. Reviewed global and cohort explainability of our model\n",
        "\n",
        "Though we covered a lot of ground, this is just scratching the surface of what the Arize platform can do. We urge you to explore more of Arize, either on your own or through one of our many other tutorials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRzcP2z9d7Ju"
      },
      "source": [
        "# About Arize\n",
        "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
        "- Automated ML monitoring and model monitoring\n",
        "- Workflows to troubleshoot model performance\n",
        "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
        "- Model prediction cohort analysis\n",
        "- Pre-deployment model validation\n",
        "- Integrated model explainability\n",
        "\n",
        "### Website\n",
        "Visit Us At: https://arize.com/model-monitoring/\n",
        "\n",
        "### Additional Resources\n",
        "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
        "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
        "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
        "- [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/)\n",
        "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
        "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
        "- [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/)\n",
        "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
        "\n",
        "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzpoTbOhcjbG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Churn Use Case.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ec6620dbdb1ae7ae605320a7ce7472136ac4f41aa18fe452eefaab7683971943"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit (conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
