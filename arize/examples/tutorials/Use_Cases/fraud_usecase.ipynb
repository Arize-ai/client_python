{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
    "\n",
    "# Getting Started with the Arize Platform - Credit Card Fraud in the Banking Industry\n",
    "\n",
    "**You are responsible for a credit card fraud model at a large bank or payment processing company. You have been alerted by a spike in credit card chargebacks leading you to suspect that fraudsters are getting away with commiting fraud undetected!** Realizing that this flaw in your model's performance has a heavy cost on your company and customers, you understand the need for a powerful toolset to troubleshoot (and prevent) costly model degradations. You turn to Arize to find out what changed in your credit card fraud detection model and how you can improve it.\n",
    "\n",
    "In this walkthrough, we are going to investigate your production credit card fraud model. We will validate degradation in model performance, troubleshoot the root cause, and furthermore set up proactive monitors to mitigate the impact of future degradations.  \n",
    "\n",
    "We will set up monitors to practively identify when our fraud model is not perfoming as expected, troubleshoot why we're seeing this deviation in production, and come up with actionable steps to improve the model.**\n",
    "\n",
    "You manage the credit card fraud detection model for a large bank or payment processing company. One day you're alerted to a huge spike in credit card chargebacks â€”fraudsters are getting away with using people's credit cards without being detected! \n",
    "\n",
    "Our steps to resolving this issue will be:\n",
    "1. Get our model onto the Arize platform to investigate\n",
    "2. Setup a performance dashboard to look at prediction performance\n",
    "3. Understand where the model is underperforming\n",
    "4. Discover the root cause of why a slice (grouping) of predictions is underperforming\n",
    "5. Set up pro-active monitoring to mitigate the impact of such degradations in the future\n",
    "\n",
    "The production data contains 1 month of data where 2 main issues exist. You will work on identifying these issues over the course of this exercise.\n",
    "\n",
    "1. An upstream data source has introduced bad (null) values for ENTRY_MODE \n",
    "2. The model is missing fraud for certain merchant types, especially in certain regions. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 0. Setup and Getting the Data\n",
    "\n",
    "The first step is to load our preexisting dataset which includes training and production environments for our creditcard fraud model. Using a preexisting dataset illustrates how simple it is to get started with the Arize platform."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Dependencies and Import Libraries ðŸ“š"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install arize -q\n",
    "!pip install tables --upgrade -q\n",
    "\n",
    "from arize.utils.types import ModelTypes, Environments\n",
    "from arize.pandas.logger import Client, Schema\n",
    "\n",
    "import pandas as pd\n",
    "import datetime, uuid, tempfile\n",
    "from datetime import timedelta\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **ðŸŒ Download the Data**\n",
    "In this walkthrough, weâ€™ll be sending real historical data (with privacy conscious changes to feature names and values). Note, that while feature names and values are made explicit in this dataset, you can achieve the same level of ML Observability using obfuscated features. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Features Description \n",
    "\n",
    "1. **MEAN_AMOUNT**: mean transaction amount for given card\n",
    "2. **STD_AMOUNT**: standard deviation from mean transaction amount \n",
    "4. **STATE**: US state where the terminal/POS system used for the transaction resides \n",
    "5. **MERCHANT_TYPE**: merchant classification category \n",
    "6. **ENTRY_MODE**: mode of card entry for the transaction \n",
    "7. **TX_AMOUNT**: transaction amount\n",
    "8. **TX_ID**: unique transaction identifier\n",
    "9. **TX_TIME**: transaction timestamp \n",
    "10. **VISA_SCORE**: Visa fraud scoring service for merchants \n",
    "11. **MASTERCARD_SCORE**: Mastercard fraud scoring service for merchants \n",
    "12. **AMEX_SCORE**: AmEx fraud scoring service for merchants \n",
    "13. **PREDICTION**: predicted fraud or not fraud\n",
    "14. **PREDICTION_SCORE**: predicted probability of fraud/not fraud\n",
    "15. **ACTUAL**: actual value, transaction was fraud or not fraud\n",
    "16. **ACTUAL_SCORE**: actual probability of a click\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this historical evaluation case, the best approach to send data into Arize is via the [Python SDK pandas logger](https://docs.arize.com/arize/api-reference/python-sdk/arize.pandas). Therefore, you will need to have a Pandas dataframe for each dataset/environment. \n",
    "\n",
    "We have **2 Environments: training and production**. Training and production are two different datasets that correspond to their respective parts of the training/production pipeline. We download each of them, storing them in a dictionary `datasets` for later use. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "environments = ['training', 'production']\n",
    "datasets = {}\n",
    "\n",
    "for environment in environments:\n",
    "\n",
    "  filepath = 'https://storage.googleapis.com/arize-assets/tutorials/use-cases/' + 'creditcard-fraud-' + environment + '.csv'  \n",
    "\n",
    "  # Create the dataframe and store in dictionary\n",
    "  datasets[environment] = pd.read_csv(filepath)\n",
    "  \n",
    "print(\"âœ… Dependencies installed and data successfully downloaded!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspect the Data \n",
    "\n",
    "Take a quick look at the dataset. The data represents a model designed and trained to evaluate the probability of a credit card transaction being fraud based on various features such as merchant_type, mean_amount, transaction amount, entry_mode, etc. The dataset contains one month of data and the performance will be evaluated by comparing:\n",
    "\n",
    "*   **PREDICTION**: The probability of a fraud transaction predicted by the model \n",
    "*   **ACTUAL**: Fraud or not fraud based on ground truth collected by credit card users\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datasets['training'][:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1. Sending Data into Arize ðŸ’«\n",
    "\n",
    "Now that we have our dataset imported, we are ready to integrate into Arize. We do this by logging (sending) important data we want to analyze to the platform. There, the data will be easily visualized and troubleshooting workflows will help us find the source of our problem.\n",
    "\n",
    "For our model, we are going to log:\n",
    "*   feature data\n",
    "*   predictions\n",
    "*   actuals\n",
    "\n",
    "## Import and Setup Arize Client\n",
    "\n",
    "The first step is to setup our Arize client. After that we will log the data.\n",
    "\n",
    "First, copy the Arize `API_KEY` and `ORG_KEY` from your admin page linked below! Copy those over to the set-up section. We will also be setting up some metadata to use across all logging.\n",
    "\n",
    "[![Button_Open.png](https://storage.googleapis.com/arize-assets/fixtures/Button_Open.png)](https://app.arize.com/admin)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ORGANIZATION_KEY = \"ORGANIZATION_KEY\"\n",
    "# API_KEY = \"API_KEY\"\n",
    "\n",
    "ORGANIZATION_KEY = \"ORGANIZATION_KEY\"\n",
    "API_KEY = \"API_KEY\"\n",
    "\n",
    "arize_client = Client(organization_key=ORGANIZATION_KEY, api_key=API_KEY)\n",
    "\n",
    "model_id = \"fraud-demo-model\"  # This is the model name that will show up in Arize\n",
    "model_version = \"v1.0\"  # Version of model - can be any string\n",
    "\n",
    "if ORGANIZATION_KEY == \"ORGANIZATION_KEY\" or API_KEY == \"API_KEY\":\n",
    "    raise ValueError(\"âŒ NEED TO CHANGE ORGANIZATION AND/OR API_KEY\")\n",
    "else:\n",
    "    print(\"âœ… Arize setup complete!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log Training & Production Data to Arize \n",
    "\n",
    "Now that our Arize client is setup, let's go ahead and log all of our data to the platform. For more details on how **`arize.pandas.logger`** works, visit out documentations page below.\n",
    "\n",
    "[![Buttons_OpenOrange.png](https://storage.googleapis.com/arize-assets/fixtures/Buttons_OpenOrange.png)](https://docs.arize.com/arize/sdks-and-integrations/python-sdk/arize.pandas)\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "*   **prediction_label_column_name**: tells Arize which column contains the predictions\n",
    "*   **actual_label_column_name**: tells Arize which column contains the actual results from field data\n",
    "*   **preidction_score_column_name**: tells Arize which column contains the prediction score from field data\n",
    "*   **actual_label_column_name**: tells Arize which column contains the actual results from field data\n",
    "*   **actual_score_column_name**: tells Arize which column contains the actual score from field data\n",
    "\n",
    "Given that our model is predicting between categories, we will use [ModelTypes.SCORE_CATEGORICAL](https://docs.arize.com/arize/product-guides-1/models/model-types) to perform this analysis.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## send training data\n",
    "df_training = datasets['training']\n",
    "\n",
    "df_training[\"prediction_ts\"] = int(datetime.datetime.now().date().strftime(\"%s\"))\n",
    "df_training[\"prediction_id\"] = [str(uuid.uuid4()) for _ in range(df_training.shape[0])]\n",
    "\n",
    "features = [\n",
    "    'MERCHANT_TYPE',\n",
    "    'ENTRY_MODE',\n",
    "    'STATE',\n",
    "    'MEAN_AMOUNT',\n",
    "    'STD_AMOUNT',\n",
    "    'TX_AMOUNT',\n",
    "    'VISA_RISK_SCORE',\n",
    "    'MASTERCARD_RISK_SCORE',\n",
    "    'AMEX_RISK_SCORE',\n",
    "]\n",
    "\n",
    "# Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
    "training_schema = Schema(\n",
    "    prediction_id_column_name=\"prediction_id\",\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_label_column_name=\"PREDICTION\",\n",
    "    prediction_score_column_name=\"PREDICTION_SCORE\",\n",
    "    actual_label_column_name=\"ACTUAL\",\n",
    "    actual_score_column_name=\"ACTUAL_SCORE\",\n",
    "    feature_column_names=features\n",
    ")\n",
    "\n",
    "# Logging Training DataFrame\n",
    "training_response = arize_client.log(\n",
    "    dataframe=df_training,\n",
    "    path=\"inferences.bin\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    batch_id=None,\n",
    "    model_type=ModelTypes.SCORE_CATEGORICAL,\n",
    "    environment=Environments.TRAINING,\n",
    "    schema=training_schema,\n",
    ")\n",
    "\n",
    "# If successful, the server will return a status_code of 200\n",
    "if training_response.status_code != 200:\n",
    "    print(f\"logging failed with response code {training_response.status_code}, {training_response.text}\")\n",
    "else:\n",
    "    print(f\"âœ… You have successfully logged training set to Arize\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log Production Data to Arize\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# changing dates for ease of visualization / to mimic recent produciton dataset \n",
    "END_DATE = datetime.date.today().strftime('%Y-%m-%d')\n",
    "START_DATE = (datetime.date.today() - timedelta(31)).strftime('%Y-%m-%d')\n",
    "\n",
    "df_production = datasets['production']\n",
    "\n",
    "def _setPredictionIDandTime (df, start, end):\n",
    "    out_df = pd.DataFrame()\n",
    "    dts = pd.date_range(start, end).to_pydatetime().tolist()\n",
    "    for dt in dts:\n",
    "        day_df = df.loc[df[\"day_of_month\"] == dt.day].copy()\n",
    "        day_df[\"prediction_ts\"] = int(dt.strftime(\"%s\"))\n",
    "        out_df = pd.concat([out_df, day_df], ignore_index=True)\n",
    "    out_df[\"prediction_id\"] = [str(uuid.uuid4()) for _ in range(out_df.shape[0])]\n",
    "    return out_df.drop(columns=\"day_of_month\")\n",
    "\n",
    "df_production = _setPredictionIDandTime(df_production, START_DATE, END_DATE)\n",
    "\n",
    "production_schema = Schema(\n",
    "    prediction_id_column_name=\"prediction_id\",\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_label_column_name=\"PREDICTION\",\n",
    "    prediction_score_column_name=\"PREDICTION_SCORE\",\n",
    "    actual_label_column_name=\"ACTUAL\",\n",
    "    actual_score_column_name=\"ACTUAL_SCORE\",\n",
    "    feature_column_names=features\n",
    ")\n",
    "\n",
    "production_response = arize_client.log(\n",
    "    dataframe=df_production,\n",
    "    path=\"inferences.bin\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    batch_id=None,\n",
    "    model_type=ModelTypes.SCORE_CATEGORICAL,\n",
    "    environment=Environments.PRODUCTION,\n",
    "    schema=production_schema,\n",
    ")\n",
    "\n",
    "if production_response.status_code != 200:\n",
    "    print(f\"logging failed with response code {production_response.status_code}, {production_response.text}\")\n",
    "else:\n",
    "    print(f\"âœ… You have successfully logged production set to Arize\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2. Confirm Data in Arize âœ…\n",
    "\n",
    "Note that the Arize performs takes about 10 minutes to index the data. While the model should appear immediately, the data will not show up until the indexing is complete. Feel free to go grab a cup of coffee as Arize works its magic! â˜•ðŸ”®\n",
    "\n",
    "**âš ï¸ DON'T SKIP:**\n",
    "In order to move on to the next step, make sure your actuals and training/production sets are loaded into the platform. To check:\n",
    "1. Navigate to models from the left bar, locate and click on model **credit-card-fraud**\n",
    "2. On the **Overview Tab**, make sure you can see Predictions and Actuals under the **Model Health** section. Once production actuals have been fully recorded on Arize, the row title will change from **0 Actuals** to **Actuals** with summary statistics such as cardinality listed in the tables.\n",
    "3. Verify the list of **Features** below **Actuals**.\n",
    "\n",
    "![image.png](https://storage.googleapis.com/arize-assets/tutorials/use-cases/checkdata.gif)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 3. Set up Model Baseline & Managed Monitors\n",
    "\n",
    "Now that our data has been logged into the [Arize platform](https://app.arize.com/) we can begin our investigation into our poorly performing fraud detection model. \n",
    "\n",
    "Arize will guide you through setting up a **Baseline** (reference environment for comparison) and automatically create **Monitors** for your model in just a few clicks â€”just follow the blue banner at the top of the page titled \"Finish setting up your model\". \n",
    "\n",
    "![image.png](https://storage.googleapis.com/arize-assets/fixtures/Click-Through%20Rate%20Use-Case/images/initial_setup_banner.png)\n",
    "\n",
    "Arize can automatically configure monitors that are best suited to your data. From the banner at the top of the screen, select the following configurations after clicking the 'Set up Model' button: \n",
    "\n",
    "1. Datasets: `Training Version 1.0`\n",
    "2. Default Metric: `False Negative Rate`, Trigger Alert When: `False Negative Rate is above .2`, Positive Class: `FRAUD`\n",
    "3. Turn On Monitoring: Drift âœ…, Data Quality âœ…, Performance âœ… \n",
    "\n",
    "You will now see that the baseline has been set and **Drift**, **Data Quality**, and **Performance** monitors have been created!!! \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/setup.gif)\n",
    "\n",
    "To prevent expensive chargebacks from getting by our models undetected Arize automatically set up monitors to ensure our model is flagged if it Performance, Data Quality, or Drift spikes above a certain threshold/before it becomes a major issue. You can see filter monitors by category, edit evaluation windows, thresholds, etc. and create custom monitors by visiting the **Monitors** tab.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4. Exploring Drift \n",
    "\n",
    "Selecting the **Drift** tab we notice that despite the increase in credit card chargebacks, we do not see any noticeable model prediction drift in production compared to our training dataset (baseline).  \n",
    "It's important to note that while we do not see any prediction drift, we do notice an increase in false negative rate at around day 5 and 13. This leads us to believe that our model is missing fraud that it has not seen in training. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/fnr.png)\n",
    "\n",
    "\n",
    "\n",
    "Let's take a look at our actual ground truth data to see if it varies from what we expect to see based on our training set... Navigate to the model **Overview** page and under **Actuals** click on **actual class**. Immediately we see that the actual data we are able to collect is drifting significantly in waves (fraud pattern?) from our training set. By taking a look at the **Distribution Comparison** beneath the **Drift over Time** widget we notice that in our training dataset (Baseline Distribution) we expect to see 1% `FRAUD` whereas in our production dataset (Current Distribution) we see highs of almost 10% `FRAUD`! \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/actualdrift.png)\n",
    "\n",
    "This confirms our hypothesis that fraud is getting through our model undetected! \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5. Setting up a proactive custom monitor \n",
    "\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/fnrmonitor.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 6. Analyzing Root Cause \n",
    "\n",
    "Arize facilitates troubleshooting which features and more specifically slices (feature/value combinations) could be the culprit of our model's performance degradation. Arize provides a number of flexible dashboard templates for users to get started in creating custom layouts for surfacing model insights. Let's see how in just a few clicks we can quickly identify which features could be causing our model to miss these fraudulent transactions. Navigate to the **Templates** section on the left sidebar and scroll down to click on the **Feature Performance Heatmap**. From there select your model, select the features you care to investigate, metric `False Negative Rate`, and positive class `FRAUD`. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/perfheatmap.gif)\n",
    "\n",
    "Arize automatically discovers and surfaces model performance issues across all features and various feature/value combinations. Visual indicators facilitate drill down analysis of the most problematic slices affecting your overall model performance. \n",
    "\n",
    "Some of the immediate insights surfaced from our **Feature Performance Heatmap** are detailed below. \n",
    "1. Worst performing `ENTRY_MODE` includes `_NULL`, followed by `ecommerce`, and `credentials_on_file`\n",
    "2. Worst performing `MERCHANT_TYPE` includes `Online Food Delivery` and `Tax Payments`\n",
    "3. Worst performing `STATE` includes `FL`, `CA`, and `NY`\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/1heatmap.png)\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/Screen%20Shot%202021-10-03%20at%206.50.16%20PM.png)\n",
    "\n",
    "Additionally we notice that the model's false negative rate is worst when the features `RISK_SCORE_*` are between `36` and `70`. Note the risk features are measurements of risk based various payment processing merchants, i.e. low risk score = low probability of fraud, high risk score = high probability of fraud. Hence, this mentally makes sense as our model is getting confused with risk scores that fall somewhere in the middle as shown below. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/riskscore.png )\n",
    "\n",
    "We can further dive into a performance analysis of our model by filtering on various low performing feature/value slices. This will help determine the data needed to upsample and retrain our model to improve performance on these segments. For example, by adding a filter in our **Feature Performance Heatmap** we can narrow down where this type of undetected fraud is originating from and through what entry modes it is most prominent. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/feature%20filtesr%20.png)\n",
    "\n",
    "In the case of `Tax Payments`, we notice that most undetected fraud is originating from a `_NULL` entry mode type and especially prominent in `NY` and `FL`. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 7. Feature Performance Insights --> Actions \n",
    "\n",
    "Using the insights gathered from our **Feature Performance Heatmap**, we can take action to 1) prevent this type of fraud abuse from going undetected in the future and 2) use our findings to improve our model. As noted previously, the slice (feature/value combination) `ENTRY_MODE: _NULL` was one of the worst performing segments for our model. Navigate back to the model **Overview** page and click into the `ENTRY_MODE` feature under the **Features** section. \n",
    "\n",
    "Upon first glance, we see that `ENTRY_MODE` is starting to drift around day 15. Furthermore, we noice that our training dataset (Baseline distribution) did not have any `_NULL` values. Clicking on the **Data Quality** tab, we can confirm the cardinality of the feature increased on the 14th day and fell back down 15 days later. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/entrymode.png)\n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/dquality.png)\n",
    "\n",
    "Similarly, by navigating to `MERCHANT_TYPE` we notice significant drift across training and production distributions across `Online Food Delivery` and `Tax Payments`. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/merch.png)\n",
    "\n",
    "This troubleshooting flow has lead us to understand that retraining the model on these slices of feature/value combinations (namely `ENTRY_MODE`:`_NULL` and `MERCHANT_TYPE`: `Online Food Delivery` and `Tax Payments`) could significantly improve our model's False Negative Rate performance. Additionally, we can take offline action to terminate the POS terminals in `FL`, `CA`, and `NY` where this spike of fraud is originating from. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 8. Model Performance Overview\n",
    "\n",
    "As we continue to check in and improve our model's performance, we want to be able to quickly and efficiently view all our important model metrics in a single pane. In the same way we set up a **Feature Performance Heatmap** we will create a **Model Performance Dashboard** to view our model's most important metrics in a single configurable layout. \n",
    "\n",
    "Navigate to the **Templates** section on the left sidebar and scroll down to click on the **Scored Model**. From there select your model, features you care to investigate, and positive class `FRAUD`. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/scoreperfoverview.gif)\n",
    "\n",
    "In addition to the default widgets Arize sets up for your dashboard, you can configure custom metrics your team cares about. In only a few clicks I added a few widgets to give me a single glance view of my model's **Accuracy**, **False Positive Rate**, and **False Negative Rate** as standalone statistics widgets. To visualize these metrics over time I also created a timeseries widget and overlayed three plots showcase the fluctuation of my metrics over time. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/customchat.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 9. Business Metrics \n",
    "Sometimes, we need metrics other than traditional statistical measures to define model performance. Business Impact is a way to measure your score model's Payoff at different thresholds (i.e, decision boundary for a score model). The Arize platform allows you to enter custom formulas, mapping model performance to your definition of model performance. Navigate to the **Business Impact** tab to set up a custom formula used to calculate the business impact of our model's performance to the overall profit/loss of your company. For example, the diagram below estimates the profit/loss of a decision made by our model. \n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/bizimpact1.png)\n",
    "\n",
    "Capturing this in our **Business Impact** tab visualizes the profit/loss based on our model's prediction threshold for fraud classification. \n",
    "\n",
    "![image](https://storage.googleapis.com/arize-assets/tutorials/use-cases/bizimpactform.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ðŸ“š Conclusion \n",
    "In this walkthrough we've shown how Arize can be used to log prediction data for a model, pinpoint model performance degradations, set up monitors to proactively catch future issues, create dashboards for at a glance model understanding, and calculate business impact metrics through custom formulas. \n",
    "\n",
    "We completed the following tasks: \n",
    "1. Uploaded data from a credit card transaction fraud model \n",
    "2. Set up a model baseline (Training V1.0) and managed Performance, Data Quality, and Drift monitors\n",
    "3. Created a Feature Performance Heatmap to surface low performing cohorts impacting our model which helped narrow down our undetected fraudulent transactions to certain `ENTRY_MODE`s and `MERCHANT_TYPE`s coming from a few specific `STATE`s.\n",
    "4. Identified correlations between our model's degrading performance with individual feature drift and distribution variance\n",
    "5. Created a model Performance Dashboard to visualize important metrics at a glance with custom timeseries metrics for ongoing analysis \n",
    "6. Captured our potential profit/loss based on our model's classification threshold using the Business Impact tab\n",
    "\n",
    "We identified the following areas of concern: \n",
    "1. `_NULL` values for the feature `ENTRY_MODE`. This leads us to believe that we need to upsample this type of feature/value combination or fix an upstream datasource to omit this undetermined value. \n",
    "2. Suspicious activity coming from `MERCHANT_TYPE`: `Online Food Delivery` and `Tax Payments`. Specifically we noticed drift of these two categories fluctuating on certain dates (weekends?) and coming from a small subset of `STATES`, namely `FL`, `CA`, and `NY`. We must look into the POS systems across these states to understand what type of organized fraud is happening from these regions while retraining our model to detect anomalous food delivery and erroneous tax payment transactions. \n",
    "3. We also noticed that our model performs poorly on transactions where the `RISK_SCORE_*` feature values falls between 35-70. Perhaps a new model strategy is needed to form stronger inferences based on these closely related median risk score values. \n",
    "\n",
    "Though we covered a lot of ground, this is just scratching the surface of what the Arize platform can do. We urge you to explore more of Arize, either on your own or through one of our many other tutorials."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# About Arize\n",
    "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
    "- Automated ML monitoring and model monitoring\n",
    "- Workflows to troubleshoot model performance\n",
    "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
    "- Model prediction cohort analysis\n",
    "- Pre-deployment model validation\n",
    "- Integrated model explainability\n",
    "\n",
    "### Website\n",
    "Visit Us At: https://arize.com/model-monitoring/\n",
    "\n",
    "### Additional Resources\n",
    "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
    "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
    "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
    "- [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/)\n",
    "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
    "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
    "- [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/)\n",
    "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
    "\n",
    "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring.\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('3.7.3': pyenv)"
  },
  "interpreter": {
   "hash": "b44196b46fa1546c6b201d55f87a4966492ed616ef4751c0443da1a75403d9e6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}