{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ce2be5_0_3L"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
        "\n",
        "# **Arize and MLflow Walkthrough**\n",
        "\n",
        "Let's get started on using Arize with MLflow! ‚ú®\n",
        "\n",
        "**Arize** and **MLflow** are MLOps tools that aim to improve connected, but different parts of your ML pipeline and ML workflow. \n",
        "\n",
        "**MLflow** is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, model registry. **Arize** is an observability & monitoring tool that helps you pre-launch validate those model experiments & versions, and allows you to benchmark, monitor, and visualize your production model performance, data drift, data quality, and explainability after it has been deployed in production.\n",
        "\n",
        "By integrating Arize and MLflow, you will be able to train, manage, and register the best models with reproducible results with Arize. At the same time, by using lightweight integrations at different stages of your ML Life Cycle (training & Serving), you can continuously monitor and ensure those performances tracked by MLflow is preserved in production.\n",
        "\n",
        "\n",
        "## ‚úîÔ∏è Steps for this Walkthrough\n",
        "1. Examples of setting up Arize and MLflow\n",
        "2. Experiment Managing with MLflow + Production Benchmarking with Arize\n",
        "3. Storing and Loading model with MLflow\n",
        "4. Integrating Serving End-point with Arize + MLflow\n",
        "5. Some key take-aways for the joint value add of using two platforms together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cchZttOJ-fFZ"
      },
      "source": [
        "# Step 0. Setup and Getting the Data\n",
        "We import the iris dataset and fit an Multi-Layered Perceptron classification model. We will be showing how to track experiments with MLflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies and Import Libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvPvFtN9tnBO"
      },
      "outputs": [],
      "source": [
        "!pip install -q arize\n",
        "!pip install -q mlflow\n",
        "\n",
        "import uuid\n",
        "import warnings\n",
        "\n",
        "import mlflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from arize.pandas.logger import Client, Schema\n",
        "from arize.utils.types import Environments, ModelTypes\n",
        "from mlflow.models.signature import ModelSignature, infer_signature\n",
        "from mlflow.types.schema import ColSpec, Schema\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **üåê Download the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n",
        "X, y = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"]), iris[\"target\"]\n",
        "X_train, X_prod, y_train, y_prod = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7fnmkBJ_uWr"
      },
      "source": [
        "# Step 1.  Sending Data into Arize üí´\n",
        "\n",
        "First, copy the Arize `API_KEY` and `SPACE_KEY` from your admin page linked below!\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.jpeg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JID6qZU-GDS",
        "outputId": "c502fe98-daa8-44d4-960e-6c24a6713efd"
      },
      "outputs": [],
      "source": [
        "ArizeSchema = Schema  # Schema is a name clash later, so we store it here\n",
        "\n",
        "SPACE_KEY = \"SPACE_KEY\"\n",
        "API_KEY = \"API_KEY\"\n",
        "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
        "\n",
        "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"‚ùå NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"‚úÖ Arize setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKkrfJbIxqHL"
      },
      "source": [
        "## Setting-up MLflow Run Record URI\n",
        "For this example, we will start a tracking location to the colab environment. You can set your tracking environment to your own machine, a SQLAlchemy compatible database, or a remote server. To read about alternative based on your training environment, [read here](https://www.mlflow.org/docs/latest/tracking.html#where-runs-are-recorded)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2utR5ne9bBM",
        "outputId": "5d382a25-3928-459d-d0c3-c11242117824"
      },
      "outputs": [],
      "source": [
        "run_uri = \"file:///content/mlruns\"\n",
        "mlflow.set_tracking_uri(run_uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qEfGXS4yDTD"
      },
      "source": [
        "# Step 2. Experiment Managing with MLFlow + Production Benchmarking with Arize\n",
        "### Integrating MLFlow and Arize can be easily integrated starting at the training stage!\n",
        "\n",
        "\n",
        "**MLflow Tracking** provides an API for managing and logging model parameters, model version, evaluation metrics, and outputs across different training runs. The MLflow UI is useful to help you organize and find the best model before deployment.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/MLflow/mlflow-tracking.png\" width=\"800\">\n",
        "\n",
        "You can easily integrate **MLFlow Tracking** with **Arize** to visualize performance benchmarks and data quality across model types and versions to validate model readiness for production.\n",
        "\n",
        "Let's first use a very simple example of logging of training metrics and parameters using mlflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRizpXsDBDRA",
        "outputId": "21d6dd60-ef7b-45cf-d6f0-a8432c4e84c7"
      },
      "outputs": [],
      "source": [
        "tags = {\"model_id\": \"mlp-classifier\", \"version\": \"v1.0\"}\n",
        "params = {\n",
        "    \"hidden_layer_sizes\": 3,\n",
        "    \"activation\": \"relu\",\n",
        "    \"learning_rate_init\": 0.001,\n",
        "}\n",
        "metrics = {\n",
        "    \"mse\": mean_squared_error,\n",
        "    \"mae\": mean_absolute_error,\n",
        "}\n",
        "# start a new run with MLFlow tags\n",
        "if mlflow.active_run() != None:\n",
        "    mlflow.end_run()\n",
        "mlflow.start_run(tags=tags)\n",
        "\n",
        "with mlflow.active_run() as run:\n",
        "    # Tracking experiment to MLflow Tracking\n",
        "    model = MLPClassifier(\n",
        "        hidden_layer_sizes=params[\"hidden_layer_sizes\"],\n",
        "        activation=params[\"activation\"],\n",
        "        learning_rate_init=params[\"learning_rate_init\"],\n",
        "    ).fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    for key in params.keys():\n",
        "        mlflow.log_param(key, params[key])\n",
        "    for key in metrics.keys():\n",
        "        error = metrics[key](y_train_pred, y_train)\n",
        "        mlflow.log_metric(key, error)\n",
        "print(\"‚úÖ Success fully tracked aggregate metric and parameter to MLflow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlWwAwJpB31V"
      },
      "source": [
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/MLflow/api-compare.png\" width=\"900\">\n",
        "\n",
        "By using `mlflow.run.tags`, we can create a one-to-one correspondance between `model_id` and `model_version` on MLflow and Arize. By including the `run.info.run_id` of each training run as part of your model `batch_id`, we make each training environment discoverable on the Arize platform in the future when the model is deployed in production post training.\n",
        "```\n",
        "# Example of tracking MLflow model params to Arize\n",
        "arize_client.log(\n",
        "    ...\n",
        "    model_id=run.data.tags['model_id'],\n",
        "    model_version=run.data.tags['model_version'],\n",
        "    batch_id=f'training {run.info.experiment_id}'\n",
        "    ...\n",
        "    environment=Environments.TRAINING,\n",
        "    ...\n",
        ")\n",
        "```\n",
        "See below for an example of matching MLflow Tracking with Arize!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUvAKGLZ8RL-",
        "outputId": "d7113d92-16b6-465d-b95f-986e23a23509"
      },
      "outputs": [],
      "source": [
        "# Logging training to Arize\n",
        "logging_df = X_train.copy()\n",
        "logging_df[\"prediction_ids\"] = pd.Series(\n",
        "    (str(uuid.uuid4()) for _ in range(len(X))), index=X.index\n",
        ")\n",
        "logging_df[\"predictions\"] = y_train_pred\n",
        "logging_df[\"actuals\"] = y_train\n",
        "\n",
        "# Defining a schema to log training records\n",
        "schema = ArizeSchema(\n",
        "    feature_column_names=X.columns,\n",
        "    prediction_id_column_name=\"prediction_ids\",\n",
        "    prediction_label_column_name=\"predictions\",\n",
        "    actual_label_column_name=\"actuals\",\n",
        ")\n",
        "\n",
        "# We use the MLflow experiments to track to Arize\n",
        "train_response = arize_client.log(\n",
        "    dataframe=logging_df,\n",
        "    model_id=run.data.tags[\"model_id\"],\n",
        "    model_version=run.data.tags[\"version\"],\n",
        "    batch_id=f\"training {run.info.experiment_id}\",\n",
        "    model_type=ModelTypes.NUMERIC,\n",
        "    environment=Environments.TRAINING,\n",
        "    schema=schema,\n",
        ")\n",
        "print(\"‚úÖ Success fully logged training evaluations and parameter to Arize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeMBSMvA84oX"
      },
      "source": [
        "## **Takeaways**: Why track your model to both MLflow and Arize?\n",
        "Logging/tracking your training & validation data to both platforms offers many joint value to your ML workflow.\n",
        "\n",
        "1. MLFlow collects **aggregate metrics** (error across entire training/validation set). Arize is an **evaluation store**, allowing you to check error rate across slices of your data (i.e where features are less than a specific value).\n",
        "2. MLFlow allows you to track **training benchmark** metrics in training and validation environment, so you can tune your parameters knowing your experiements won't be lost. Arize tracks the same training/validation data, calculates the same metrics, and uses it as the **production baseline** later to ensure that they are preserved in production.\n",
        "3. Matching `run_id` with `batch_id` allows for best practice reproducibilty, so your training and validation environments are always reproducible and visualized on Arize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4s7sMnwJMdY"
      },
      "source": [
        "# Step 3. Storing and Loading Model in Production\n",
        "A MLflow Model is a standard format for packaging machine learning models that can be used in a variety of downstream tools. In this example, we show an example of how to us the `mlflow.schema` to store and load our `sklearn` model.\n",
        "\n",
        "The model directory packaging can then be used in production, whether through REST API or through another serving end-point such as SageMaker, Azure, Algorithmia, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHCoMtamI8gl",
        "outputId": "9d2863b2-44f2-4724-cf8a-442724a95923"
      },
      "outputs": [],
      "source": [
        "MLflowSchema = Schema  # Since there is a name clash in importS\n",
        "\n",
        "conda_env = {\n",
        "    \"channels\": [\"conda-forge\"],\n",
        "    \"dependencies\": [\"python=3.8.8\", \"pip\"],\n",
        "    \"pip\": [\n",
        "        \"mlflow\",\n",
        "        \"scikit-learn==0.23.2\",\n",
        "    ],\n",
        "    \"name\": \"mlflow-env\",\n",
        "}\n",
        "\n",
        "input_schema = MLflowSchema(\n",
        "    [\n",
        "        ColSpec(\"double\", \"sepal length (cm)\"),\n",
        "        ColSpec(\"double\", \"sepal width (cm)\"),\n",
        "        ColSpec(\"double\", \"petal length (cm)\"),\n",
        "        ColSpec(\"double\", \"petal width (cm)\"),\n",
        "    ]\n",
        ")\n",
        "output_schema = MLflowSchema([ColSpec(\"long\")])\n",
        "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
        "\n",
        "# Storing model in your active r\n",
        "mlflow.sklearn.log_model(\n",
        "    model, \"model_directory\", conda_env=conda_env, signature=signature\n",
        ")\n",
        "print(\"‚úÖ Success stored model in current run directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vwIaVHcSotJ",
        "outputId": "e82acbdd-f4a0-42c1-a2c8-73439757d30b"
      },
      "outputs": [],
      "source": [
        "# Reloading from your run_uri directory\n",
        "# This also works if you are using a remote directory\n",
        "production_model = mlflow.sklearn.load_model(\n",
        "    run_uri + f\"/0/{mlflow.active_run().info.run_id}/artifacts/model_directory\"\n",
        ")\n",
        "production_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9vMbdmyDkvh"
      },
      "source": [
        "# Step 4. Integrating Serving End-Point with MLflow Signature and Arize Schema\n",
        "\n",
        "**MLflow Signature** is used to specify input (and output) format of your data, enforcing shapes, data types, and/or feature names. The input requirements are checked before calls are made to the underlying model (i.e `sklearn` model), allowing for another abstraction layer to avoid production issues of input mismatch.\n",
        "\n",
        "**Arize Schema** is used by the Arize SDK to specify model and input data format such as feature column names and prediction column name. It can be easily integrated with the **MLflow Signature** approach, allowing your model schema to be automatically detected as you version your model.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/MLflow/signature_inference.png\" width=\"900\">\n",
        "\n",
        "The `ColSpec` abstraction of feature columns allows for Arize to detect and construct the `Schema` object at the production serving end-point. \n",
        "\n",
        "## Usage Case Example\n",
        "\n",
        "Suppose for model version 2.0, **you add or remove features** for your MLflow model. When you change your **MLflow Signature**, and Arize can directly infer and understand the change in your model Schema, making production monitoring pipeline painless to manage. The segment of code below is an example of how to allow Arize to automatically infer model structure using the MLflow Signature. \n",
        "\n",
        "**Note: This also ensures that the feature names with exact one-to-one mapping on both MLflow and Arize.**\n",
        "\n",
        "In this example, we used the function `predict_and_track` to both predict and log automatically to Arize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryF-A6y_QM6Z"
      },
      "outputs": [],
      "source": [
        "def predict_and_track(model, model_version, X_prod, prediction_ids):\n",
        "    \"\"\"\n",
        "    Example function that shows how to integrate Arize with MLflow Signature\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_prod)\n",
        "    # Step 1: Discover model schema from MLFlow\n",
        "    signature = infer_signature(X_prod, y_pred)\n",
        "    # extract the feature columns from our signature\n",
        "    feature_column_names = signature.inputs.column_names()\n",
        "    schema = ArizeSchema(\n",
        "        feature_column_names=feature_column_names,\n",
        "        prediction_id_column_name=\"prediction_ids\",\n",
        "        prediction_label_column_name=\"predictions\",\n",
        "    )\n",
        "    # Step 2: Log to Arize with the same schema\n",
        "    logging_df = X_prod.copy()\n",
        "    logging_df[\"prediction_ids\"] = prediction_ids\n",
        "    logging_df[\"predictions\"] = y_pred\n",
        "    logging_response = arize_client.log(\n",
        "        dataframe=logging_df,\n",
        "        model_id=\"mlp-classifier\",\n",
        "        model_version=model_version,\n",
        "        model_type=ModelTypes.NUMERIC,\n",
        "        environment=Environments.PRODUCTION,\n",
        "        schema=schema,\n",
        "    )\n",
        "    # Step 3: Return production predictions\n",
        "    print(\n",
        "        f\"‚úÖ Successfully tracked model with {len(feature_column_names)} features: {feature_column_names}\"\n",
        "    )\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCp-aa0nUkYP"
      },
      "source": [
        "Here, we use our originally defined Schema object. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQxGqe6KMPmY",
        "outputId": "54d6ec05-bc98-4dd7-e61e-141c4753f244"
      },
      "outputs": [],
      "source": [
        "# Reusuing the same Schema\n",
        "input_schema = MLflowSchema(\n",
        "    [\n",
        "        ColSpec(\"double\", \"sepal length (cm)\"),\n",
        "        ColSpec(\"double\", \"sepal width (cm)\"),\n",
        "        ColSpec(\"double\", \"petal length (cm)\"),\n",
        "        ColSpec(\"double\", \"petal width (cm)\"),\n",
        "    ]\n",
        ")\n",
        "output_schema = MLflowSchema([ColSpec(\"long\")])\n",
        "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
        "\n",
        "# Saving and reloading model\n",
        "mlflow.sklearn.log_model(\n",
        "    model, \"model_directory\", conda_env=conda_env, signature=signature\n",
        ")\n",
        "loaded_model = mlflow.sklearn.load_model(\n",
        "    run_uri + f\"/0/{mlflow.active_run().info.run_id}/artifacts/model_directory\"\n",
        ")\n",
        "\n",
        "# Simulate production environment with a new v2.0 model\n",
        "prediction_ids = pd.Series(\n",
        "    (str(uuid.uuid4()) for _ in range(len(X_prod))), index=X_prod.index\n",
        ")\n",
        "y_prod_pred = predict_and_track(model, \"production v2.0\", X_prod, prediction_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69gsCtwlUzyz"
      },
      "source": [
        "### **Note:** In this section, we didn't having to modify Arize Schema since it is directly inferred based on the MLflow Schema in production, only requiring you to define your MLflow Schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d6kuh1_Tb4l",
        "outputId": "18d77c14-850b-4a23-bf1a-ec76d5ba5bf2"
      },
      "outputs": [],
      "source": [
        "# Fit a new model version with only 3 features\n",
        "X_train_3feature = X_prod.drop(columns=[\"petal width (cm)\"])\n",
        "X_prod_3feature = X_prod.drop(columns=[\"petal width (cm)\"])\n",
        "new_model = MLPClassifier().fit(X_train_3feature, y_train)\n",
        "\n",
        "# Redefine our Schema from MLflow\n",
        "input_schema = MLflowSchema(\n",
        "    [\n",
        "        ColSpec(\"double\", \"sepal length (cm)\"),\n",
        "        ColSpec(\"double\", \"sepal width (cm)\"),\n",
        "        ColSpec(\"double\", \"petal length (cm)\"),\n",
        "        # ColSpec(\"double\", \"petal width (cm)\") we remove this feature to simulate iterating\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Saving and reloading model\n",
        "mlflow.sklearn.log_model(\n",
        "    model, \"model_directory\", conda_env=conda_env, signature=signature\n",
        ")\n",
        "loaded_model = mlflow.sklearn.load_model(\n",
        "    run_uri + f\"/0/{mlflow.active_run().info.run_id}/artifacts/model_directory\"\n",
        ")\n",
        "\n",
        "# Simulate production environment with a new v2.0 model\n",
        "prediction_ids = pd.Series(\n",
        "    (str(uuid.uuid4()) for _ in range(len(X_prod_3feature))),\n",
        "    index=X_prod_3feature.index,\n",
        ")\n",
        "X_prod_pred_3feature = predict_and_track(\n",
        "    new_model, \"production v2.0\", X_prod_3feature, prediction_ids\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouKIhtm5yYvj"
      },
      "source": [
        "# Appendix: Integrating With Deployment Tools as Serving End-Point\n",
        "If you are using one the MLflow [Built-in Deployment Tools](https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools) such as SageMaker or Azure, you may also want to see our serving end-point integrations below.\n",
        "\n",
        "[![Buttons_OpenOrange.png](https://storage.googleapis.com/arize-assets/fixtures/Buttons_OpenOrange.png)](https://docs.arize.com/arize/integrations/integrations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waO3jlb0MRsj"
      },
      "source": [
        "# üìö **Final Takeaways**: Why use Arize with MLflow?\n",
        "Logging/tracking your training & validation data to both platforms offers many joint value to your ML workflow.\n",
        "\n",
        "1. MLFlow collects **aggregate metrics** (error across entire training/validation set). Arize is an **evaluation store**, allowing you to check error rate across slices of your data (i.e where features are less than a specific value).\n",
        "2. MLFlow allows you to track **training benchmark** metrics in training and validation environment, so you can tune your parameters knowing your experiements won't be lost. Arize tracks the same training/validation data, calculates the same metrics, and uses it as the **production baseline** later to ensure that they are preserved in production.\n",
        "3. Integrating Arize with MLflow at production serving end-point is simple. Redefining the MLflow `Schema` object each time you modify your feature names or add new features will allow Arize to infer feature information about your models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rymH8AmkCgLu",
        "outputId": "a057065e-c0ec-46cf-c3d5-484948061e21"
      },
      "outputs": [],
      "source": [
        "print(\"üéâ You are done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELn9UgMW9YYq"
      },
      "source": [
        "# Overview\n",
        "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
        "- Automated ML monitoring and model monitoring\n",
        "- Workflows to troubleshoot model performance\n",
        "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
        "- Model prediction cohort analysis\n",
        "- Pre-deployment model validation\n",
        "- Integrated model explainability\n",
        "\n",
        "### Website\n",
        "Visit Us At: https://arize.com/model-monitoring/\n",
        "\n",
        "### Additional Resources\n",
        "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
        "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
        "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
        "- [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/)\n",
        "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
        "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
        "- [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/)\n",
        "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
        "\n",
        "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Arize_Tutorial_MLflow.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
