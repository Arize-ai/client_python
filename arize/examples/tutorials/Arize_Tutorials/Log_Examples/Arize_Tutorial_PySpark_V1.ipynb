{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQj9YRzkS9St"
      },
      "source": [
        "# Tutorial: Sending PySpark DataFrame to Arize\n",
        "\n",
        "In the current version of Arize Python SDK, only Pandas DataFrames are supported. To log Spark DataFrames, which have `rdds` as their underlying structure, we will use `mapPartitions` to log them to arize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjTwoaJ_GzbZ"
      },
      "source": [
        "# Install Dependencies in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGfDxZNUW_Fe"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -q arize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldVoUMYSVKbF"
      },
      "source": [
        "# Parallelizing PySpark DataFrame\n",
        "We first create a dummy PySpark DataFrame to send.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYYYHJYhj9BP"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import Row, SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read some dummy data for logging to Arize later\n",
        "data = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/arize-assets/fixtures/compare-model-a.csv\"\n",
        ")\n",
        "df_pandas = data[[\"loan_amount\", \"interest_rate\", \"grade\", \"prediction\", \"score\"]]\n",
        "\n",
        "df_pandas = pd.concat([df_pandas] * 5)\n",
        "\n",
        "print(\"This is a pandas DataFrame:\")\n",
        "display(df_pandas)\n",
        "\n",
        "# Create PySpark dataframe unparallelized\n",
        "df_spark = spark.createDataFrame(df_pandas)\n",
        "\n",
        "print(\"\\nThis is the corresponding spark DataFrame\")\n",
        "df_spark.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7vQo-N3cggh"
      },
      "source": [
        "# Using `mapPartitions` to log each partition to Arize\n",
        "`map_func` will be applied to each partition on of spark_df, allowing local copies of `pd.DataFrame` to be made and logged to Arize.\n",
        "\n",
        "`success` will be returned if all entries in a particular partition has been logged properly. Otherwise, it will return the error code and error message for that partition instead.\n",
        "\n",
        "`map_send_arize` should send `spark_df` to Arize with at least one of: `shap, prediction_labels, actual_labels`\n",
        "\n",
        "## How To Log to Arize:\n",
        "`your_spark_df.rdd.mapPartitions(map_func).collect()`\n",
        "\n",
        "You will also need to update the `API_KEY` and `SPACE_KEY`\n",
        "### Setting up Arize Client:\n",
        "First, copy the Arize `API_KEY` and `SPACE_KEY` from your admin page linked below!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA9UAhVkAhO0"
      },
      "outputs": [],
      "source": [
        "from arize.pandas.logger import Client, Schema\n",
        "from arize.utils.types import ModelTypes, Environments\n",
        "\n",
        "SPACE_KEY = \"SPACE_KEY\"\n",
        "API_KEY = \"API_KEY\"\n",
        "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
        "\n",
        "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"❌ NEED TO CHANGE SPACE AND/OR API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTVAA--tKdrL"
      },
      "source": [
        "Define the function used by `pyspark.RDD.mapPartitions`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xTlDkWMJ9wU"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import uuid\n",
        "\n",
        "\n",
        "def map_send_arize(pyspark_df_partition: itertools.chain):\n",
        "    \"\"\"\n",
        "    Mapping function to be used to log to Arize\n",
        "    \"\"\"\n",
        "    # Step 1: iterating through each Row to create pd.DataFrame\n",
        "    pandas_df = None\n",
        "    for row in pyspark_df_partition:\n",
        "        row_dict = row.asDict()\n",
        "        row_dict[\"prediction_id\"] = str(uuid.uuid4())\n",
        "        if pandas_df is None:\n",
        "            pandas_df = pd.DataFrame(columns=row_dict.keys())\n",
        "        pandas_df.loc[len(pandas_df)] = row_dict\n",
        "\n",
        "    # Step 2: Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
        "    pandas_df_schema = Schema(\n",
        "        prediction_id_column_name=\"prediction_id\",  # REQUIRED\n",
        "        prediction_label_column_name=\"prediction\",\n",
        "        prediction_score_column_name=\"score\",\n",
        "        feature_column_names=[\"loan_amount\", \"interest_rate\", \"grade\"],\n",
        "    )\n",
        "\n",
        "    # Step 3: Log to arize\n",
        "    response = arize_client.log(\n",
        "        dataframe=pandas_df,\n",
        "        schema=pandas_df_schema,\n",
        "        model_id=\"pyspark-loan-model\",\n",
        "        model_version=\"1.0\",\n",
        "        model_type=ModelTypes.SCORE_CATEGORICAL,\n",
        "        environment=Environments.PRODUCTION,\n",
        "    )\n",
        "\n",
        "    # Step 4: Check for errors when logging\n",
        "    return iter([f\"Status {response.status_code}: {response.text}\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9LQq2PujitJ"
      },
      "source": [
        "# Logging Example\n",
        "For each `log` response we get from Arize, `collect()` will combine the iterables as the map function completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1M6GRzXS2zF"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "log_output = df_spark.rdd.mapPartitions(map_send_arize).collect()\n",
        "print(f\"number of records sent: {len(df_pandas)}\")\n",
        "print(f\"number of arize responses: {len(log_output)}\")\n",
        "if all(response.startswith(\"Status 200:\") for response in log_output):\n",
        "    print(f\"✅ all log requests are successful\")\n",
        "else:\n",
        "    for response in log_output:\n",
        "        if not response.startswith(\"Status 200:\"):\n",
        "            print(f\"❌ {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjezXxbOWcjY"
      },
      "source": [
        "# **Overview**\n",
        "\n",
        null,
        "\n",
        "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
        "- Automated ML monitoring and model monitoring\n",
        "- Workflows to troubleshoot model performance\n",
        "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
        "- Model prediction cohort analysis\n",
        "- Pre-deployment model validation\n",
        "- Integrated model explainability\n",
        "\n",
        "### Website\n",
        "Visit Us At: https://arize.com/model-monitoring/\n",
        "\n",
        "### Additional Resources\n",
        "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
        "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
        "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
        "- [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/)\n",
        "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
        "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
        "- [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/)\n",
        "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
        "\n",
        "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Arize_Tutorial_PySpark_V1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
