{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "511HJ-fHWnFv"
   },
   "source": [
    "## Send 1 million inferences with 200 features to Arize in chunks of 100K records at a time\n",
    "\n",
    "Included is sample code of how you can split large dataframes into smaller chunks prior to sending to Arize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1POG6BEfQnMG",
    "outputId": "951dfab9-b511-4962-c134-a4aaa36c6dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 25.5 MB 51.8 MB/s \n",
      "\u001b[?25hStep 1 ✅: Install Arize, you are using sdk version: 3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip -q install arize\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from arize.pandas.logger import Client, Schema\n",
    "from arize.utils.types import Environments, ModelTypes\n",
    "\n",
    "import arize\n",
    "\n",
    "print(f\"Step 1 ✅: Install Arize, you are using sdk version: {arize.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZlF6OiQQsq5"
   },
   "source": [
    "### Set up Arize Client with your API and Space Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btoJ-OY5DW5K",
    "outputId": "6a4949f0-84a4-420d-e958-402afa45199b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 ✅: Import and Setup Arize Client Done! Now we can start using Arize!\n"
     ]
    }
   ],
   "source": [
    "SPACE_KEY = \"SPACE_KEY\"\n",
    "API_KEY = \"API_KEY\"\n",
    "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY, uri=uri)\n",
    "\n",
    "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
    "    raise ValueError(\"❌ NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
    "else:\n",
    "    print(\"Step 2 ✅: Import and Setup Arize Client Done! Now we can start using Arize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uohiUGbQRH6t"
   },
   "source": [
    "### Sample functions to emulate a large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OWSc0hFn-Y4W"
   },
   "outputs": [],
   "source": [
    "def simulate_production_timestamps(X, days=30):\n",
    "    t = datetime.now()\n",
    "    current_ts, earlier_ts = t.timestamp(), (t - timedelta(days=days)).timestamp()\n",
    "    return pd.Series(np.linspace(earlier_ts, current_ts, num=len(X)), index=X.index)\n",
    "\n",
    "\n",
    "def get_feature_columns(num_cols):\n",
    "    cols = []\n",
    "    for i in range(0, num_cols):\n",
    "        cols.append(f\"feat_{i}\")\n",
    "    return cols\n",
    "\n",
    "\n",
    "def get_shap_columns(num_cols):\n",
    "    cols = []\n",
    "    for i in range(0, num_cols):\n",
    "        cols.append(f\"feat_{i}\")\n",
    "    return cols\n",
    "\n",
    "\n",
    "def generate_prediction_ids(X):\n",
    "    return pd.Series((str(uuid.uuid4()) for _ in range(len(X))), index=X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRnLGH4pRkTS"
   },
   "source": [
    "### Generate random data for a DataFrames which we will chunk on a later step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SBfeSnxzUYnP"
   },
   "outputs": [],
   "source": [
    "LABELS = [\"Item 1\", \"Item 2\", \"Item 3\"]\n",
    "\n",
    "NUM_RECORDS = 1_000_000\n",
    "NUM_FEATS = 200\n",
    "\n",
    "feat_names = get_feature_columns(NUM_FEATS)\n",
    "\n",
    "features = pd.DataFrame(\n",
    "    np.random.random(size=(NUM_RECORDS, NUM_FEATS)),\n",
    "    columns=feat_names,\n",
    ")\n",
    "\n",
    "shap_values_column_names_mapping = {f\"{feat}\": f\"{feat}_shap\" for feat in feat_names}\n",
    "\n",
    "shap_columns = [shap_values_column_names_mapping.get(n, n) for n in feat_names]\n",
    "\n",
    "shap_values = pd.DataFrame(\n",
    "    np.random.random(size=(NUM_RECORDS, NUM_FEATS)),\n",
    "    columns=shap_columns,\n",
    ")\n",
    "shap_values.rename(columns=shap_values_column_names_mapping)\n",
    "\n",
    "pLabels = [random.choice(LABELS) for i in range(NUM_RECORDS)]\n",
    "pred_labels = pd.DataFrame(\n",
    "    np.random.random(size=(NUM_RECORDS, 2)),\n",
    "    columns=[\"prediction_label\", \"prediction_score\"],\n",
    ")\n",
    "pred_labels[\"prediction_label\"] = pLabels\n",
    "\n",
    "aLabels = [random.choice(LABELS) for i in range(NUM_RECORDS)]\n",
    "actual_labels = pd.DataFrame(\n",
    "    np.random.random(size=(NUM_RECORDS, 2)), columns=[\"actual_label\", \"actual_score\"]\n",
    ")\n",
    "actual_labels[\"actual_label\"] = aLabels\n",
    "\n",
    "ids = pd.DataFrame(\n",
    "    [str(uuid.uuid4()) for _ in range(NUM_RECORDS)], columns=[\"prediction_id\"]\n",
    ")\n",
    "\n",
    "inferences = pd.concat([features, pred_labels, ids, actual_labels, shap_values], axis=1)\n",
    "\n",
    "inferences[\"prediction_ts\"] = simulate_production_timestamps(inferences, 364)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1bxrUwyR4mU"
   },
   "source": [
    "### Send data to Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrLDXNSvLuSo",
    "outputId": "419ce163-5a7e-49e5-cdc8-223ca7819a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ You have successfully logged records from index 0 to 100000 to Arize!\n",
      "✅ You have successfully logged records from index 100000 to 200000 to Arize!\n",
      "✅ You have successfully logged records from index 200000 to 300000 to Arize!\n"
     ]
    }
   ],
   "source": [
    "production_schema = Schema(\n",
    "    prediction_id_column_name=\"prediction_id\",\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_label_column_name=\"prediction_label\",\n",
    "    prediction_score_column_name=\"prediction_score\",\n",
    "    actual_label_column_name=\"actual_label\",\n",
    "    actual_score_column_name=\"actual_score\",\n",
    "    feature_column_names=feat_names,\n",
    "    shap_values_column_names=shap_values_column_names_mapping,\n",
    ")\n",
    "\n",
    "### We will chuck the dataframe into 100K records at a time\n",
    "start = 0\n",
    "stop = 100_000\n",
    "step = stop\n",
    "\n",
    "model_id = \"model_to_split\"\n",
    "model_version = \"1.0\"\n",
    "model_type = ModelTypes.SCORE_CATEGORICAL\n",
    "\n",
    "while stop <= len(inferences):\n",
    "    try:\n",
    "        response = arize_client.log(\n",
    "            dataframe=inferences.iloc[start:stop],\n",
    "            schema=production_schema,\n",
    "            model_id=model_id,\n",
    "            model_version=model_version,\n",
    "            model_type=model_type,\n",
    "            environment=Environments.PRODUCTION,\n",
    "        )\n",
    "        # If successful, the server will return a status_code of 200\n",
    "        if response.status_code != 200:\n",
    "            ## In the case a 200 was not received, you'll want to try the chunk again, so we dont increment the start/stop variables\n",
    "            print(\n",
    "                f\"❌ Logging failed with response code {response.status_code}, {response.text}, will try again\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"✅ You have successfully logged records from index {start} to {stop} to Arize!\"\n",
    "            )\n",
    "            ## If we got a 200 ACK, we can move on to the next chunk\n",
    "            start = start + step\n",
    "            stop = stop + step\n",
    "    except Exception as err:\n",
    "        print(\n",
    "            f\"An exception occurred when logging index {start} to {stop}, trying again\\n Exception: {err}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Chunking Large Datasets.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b72fb6e7218a7ba9b0ea2d899c6447cf37e05630d8cf381a78bf2982129f31d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env-01': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
