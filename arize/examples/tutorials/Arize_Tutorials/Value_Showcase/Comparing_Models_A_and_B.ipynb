{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIwsFskYR1wJ"
      },
      "source": [
        "# Getting Started with the Arize Platform - Comparing Models\n",
        "\n",
        "**In this walkthrough, we are going to look at how to use Arize to compare two models with Arize performance dashboards!**\n",
        "\n",
        "In this example, you manage the default/fraud detection model for the widely used [Lending Club](https://www.lendingclub.com/). You have been serving two models with very similar performances in traing/validation, now you would like to investigate which model will you finally use to serve customers in the end.\n",
        "\n",
        "**Specifically, you want to not only investigate model metrics such as accuracy, but you also want to understand model performance in certain segments of your customers to optimize for better customer lifetime value**, so you turn to Arize for investigation.\n",
        "\n",
        "## The Story: Low Fico Score\n",
        "\n",
        "In this example, we will choose `fico_score <= 500` as our customer segment of interest. Our data science director wants to extend the product line to be more fair for different disadvantaged groups, so correctly predicting outcome for loan in this group is incredibly important.\n",
        "\n",
        "You want to pick the best model to optimize accuracy, but you also want to optimize protecting vulnerable groups.\n",
        "\n",
        "\n",
        "## Our steps to resolving this issue will be :\n",
        "\n",
        "1. Log our models onto the Arize platform using the Python SDK\n",
        "2. Set up Dashboards to view two model performances side by side\n",
        "3. Use dashboard features to hone in on specific reasons to select one model over another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G7ZAvxhXRKW"
      },
      "source": [
        "# Step 1: Setup and Getting the Data\n",
        "### Step 1.1: Loading data from Arize client\n",
        "We will load in some pre-existing data for the Lending Club model - training data, features, predictions, and class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLfYh5Tx1BLl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import concurrent.futures as cf\n",
        "import uuid\n",
        "import datetime\n",
        "\n",
        "\n",
        "def unpack_data(data):\n",
        "    X = data.drop(columns=['label', 'prediction', 'score'])\n",
        "    y = data['label']\n",
        "    pred = data['prediction']\n",
        "    score = data['score']\n",
        "    return X, y, pred, score\n",
        "\n",
        "model_a_data = pd.read_csv('https://storage.googleapis.com/arize-assets/fixtures/compare-model-a.csv')\n",
        "model_b_data = pd.read_csv('https://storage.googleapis.com/arize-assets/fixtures/compare-model-b.csv')\n",
        "\n",
        "X_m1, y_m1, y_pred_m1, y_pred_score_m1 = unpack_data(model_a_data)\n",
        "X_m2, y_m2, y_pred_m2, y_pred_score_m2 = unpack_data(model_b_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyKv8r_mg3dQ"
      },
      "source": [
        "### Step 1.2: Setting up Arize Client:\n",
        "First, copy the Arize `API_KEY` and `SPACE_KEY` from your admin page shown below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/copy-keys.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDjEX0rJzeG4",
        "outputId": "e1e23b3b-b6ba-484b-cb7f-b05188d8a62b"
      },
      "outputs": [],
      "source": [
        "!pip install arize -q\n",
        "import concurrent.futures as cf\n",
        "from arize.pandas.logger import Client, Schema\n",
        "from arize.utils.types import ModelTypes, Environments\n",
        "\n",
        "# Step 1.2: Set-up Arize Client and model meta data\n",
        "SPACE_KEY = 'SPACE_KEY'\n",
        "API_KEY = 'API_KEY'\n",
        "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
        "\n",
        "model_type = ModelTypes.SCORE_CATEGORICAL\n",
        "\n",
        "if (SPACE_KEY == 'SPACE_KEY' or API_KEY == 'API_KEY'):\n",
        "    raise ValueError(\"❌ NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"✅ Arize setup complete!\")\n",
        "\n",
        "# Step 2: Some helpful helper functions for later\n",
        "def arize_responses_helper(responses):\n",
        "    for response in cf.as_completed(responses):\n",
        "        res = response.result()\n",
        "        if res.status_code != 200:\n",
        "            raise ValueError(f'future failed with response code {res.status_code}, {res.text}')\n",
        "\n",
        "def simulate_production_timestamps(num_entries, days=30):\n",
        "    \"\"\"\n",
        "    Takes in: number of entries used for bulk_log, and number of days we want simulate back trace\n",
        "    Returns prediction_timestamps arguement for bulk_log, uniformally distributed over time period\n",
        "    \"\"\"\n",
        "    current_time = datetime.datetime.now().timestamp()\n",
        "    earlier_time = (datetime.datetime.now() - datetime.timedelta(days=days)).timestamp()\n",
        "    optional_prediction_timestamp = np.linspace(earlier_time, current_time, num=num_entries)\n",
        "    optional_prediction_timestamp = pd.Series(optional_prediction_timestamp.astype(int))\n",
        "    return optional_prediction_timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uSqp4cfg4eo"
      },
      "source": [
        "### Step 1.3: Logging Model A and Model B to Arize\n",
        "First, we take our existing models and log them to Arize using the Python SDK. For more details on how **`arize.pandas.log`** works, visit out documentations page below.\n",
        "\n",
        "[![Buttons_OpenOrange.png](https://storage.googleapis.com/arize-assets/fixtures/Buttons_OpenOrange.png)](https://docs.arize.com/arize/sdks-and-integrations/python-sdk/arize.pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADi6XlPuYfdB",
        "outputId": "fec709db-10ff-4716-e793-08bb77c4e7f2"
      },
      "outputs": [],
      "source": [
        "# Load our predictions, score, and actuals into a pd.DataFrame for Logging\n",
        "model_a_df = X_m1.copy()\n",
        "model_a_df['prediction_ids'] = [str(uuid.uuid4()) for _ in range(len(model_a_df))]\n",
        "model_a_df['predictions'] = y_pred_m1\n",
        "model_a_df['predictions_score'] = y_pred_score_m1\n",
        "model_a_df['actuals'] = y_m1\n",
        "model_a_df['prediction_ts'] = simulate_production_timestamps(len(y_pred_m1), days=30)\n",
        "feature_column_names = X_m1.columns \n",
        "\n",
        "# Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
        "schema = Schema(\n",
        "    prediction_id_column_name=\"prediction_ids\",  # REQUIRED\n",
        "    prediction_label_column_name=\"predictions\",\n",
        "    prediction_score_column_name=\"predictions_score\",\n",
        "    actual_label_column_name=\"actuals\",\n",
        "    timestamp_column_name=\"prediction_ts\",\n",
        "    # feature_column_names should be specific to your model\n",
        "    feature_column_names=feature_column_names,\n",
        ")\n",
        "\n",
        "# arize_client.log returns a Response object from Python's requests module\n",
        "log_model_a_responses = arize_client.log(\n",
        "    dataframe=model_a_df,\n",
        "    model_id='compare-models-demo-a',\n",
        "    model_version='1.0',\n",
        "    model_type=model_type,\n",
        "    environment=Environments.PRODUCTION,\n",
        "    schema=schema,\n",
        ")\n",
        "\n",
        "arize_responses_helper(log_model_a_responses)\n",
        "print(\"✅ Logged Models A to Arize!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BaVhq-NZSx4",
        "outputId": "f5d6ca37-561f-4491-8f40-1faa8b604b3e"
      },
      "outputs": [],
      "source": [
        "# Load our predictions, score, and actuals into a pd.DataFrame for Logging\n",
        "model_b_df = X_m2.copy()\n",
        "model_b_df['prediction_ids'] = [str(uuid.uuid4()) for _ in range(len(model_b_df))]\n",
        "model_b_df['predictions'] = y_pred_m2\n",
        "model_b_df['predictions_score'] = y_pred_score_m2\n",
        "model_b_df['actuals'] = y_m2\n",
        "model_b_df['prediction_ts'] = simulate_production_timestamps(len(y_pred_m2), days=30)\n",
        "feature_column_names = X_m2.columns\n",
        "\n",
        "# Note: We do not have to redefine Schema() since two models have the same Schema()\n",
        "\n",
        "# arize_client.log returns a Response object from Python's requests module\n",
        "log_model_b_responses = arize_client.log(\n",
        "    dataframe=model_b_df,\n",
        "    model_id='compare-models-demo-b',\n",
        "    model_version='1.0',\n",
        "    model_type=model_type,\n",
        "    environment=Environments.PRODUCTION,\n",
        "    schema=schema,\n",
        ")\n",
        "\n",
        "arize_responses_helper(log_model_b_responses)\n",
        "print(\"✅ Logged Models B to Arize!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Data Ingestion Information\n",
        "\n",
        "Data will be available in the UI in about 10 minutes after it was received. If data from a new model is sent, the model will be reflected almost immediately in the Arize platform. However, you will not see data yet. To verify data has been sent correctly and is being processed, we recommend that you check our Data Ingestion tab. \n",
        "\n",
        "You will be able to see the predictions, actuals, and feature importances that have been sent in the last week, last day or last 30 minutes. \n",
        "\n",
        "An example view of the Data Ingestion tab from a model, when data is sent continuously over 30 minutes, is shown in the image below. \n",
        "\n",
        "<img src=\"https://storage.googleapis.com/arize-assets/fixtures/data-ingestion-tab.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H532GMx0hHMH"
      },
      "source": [
        "## Coffee Time ☕️\n",
        "Note that the Arize performs takes about 10 minutes to index the data. While the model should appear immediately, the data will not show up till the indexing is done. Feel free to go grab a cup of coffee as Arize works its magic! 🔮\n",
        "\n",
        "Your Prediction Volume may look slightly different!\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/waiting-on-data.png)\n",
        "\n",
        "Actual data will show up under **Model Health**. Once the number changes from **0 Actuals** to **Actuals** (with summary statistics listed in the drop-down), your production actuals will have been fully recorded on Arize!\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/waiting-on-actual-data.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk55zhegitFb"
      },
      "source": [
        "# Step 2: Set-up Model Performance Dashboard on Arize\n",
        "**Model Performance Dashboards** are customizable and flexible dashboards capable of enabling live and continuous monitoring of model statistics, time series, and distributions. With Dashboards, you can quickly set up recurring deepdive into slices of your data (i.e, segments of users, business bottlenecks, etc), serving purposes such as compare model performances, validate your prediction volumes, and many more.\n",
        "\n",
        "In this example, we will use a **Template** option as shown below for **Compare Model A with Model B**. There are other templated options depending on your model types as well (i.e regression, scored model, binary classification, multi-class, etc).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIOQsiFaYiQi"
      },
      "source": [
        "## 2.1 Setting up Initial Dashboard\n",
        "First, click on **Template** and then **Compare Model A with Model B**\n",
        "1. Set your Dashboard Title, we will call the Dashboard **Compare Performance of A and B**\n",
        "2. Select `compare-models-demo-a` and `compare-models-demo-b`\n",
        "\n",
        "You could select model versions to compare (or even compare model versions against each other). In this example, we leave model version as `All` since we only logged one version!\n",
        "\n",
        "**✏️ You can click on the gifs to replay it**\n",
        "[![image.png](https://storage.googleapis.com/arize-assets/fixtures/create-template.gif)](https://storage.googleapis.com/arize-assets/fixtures/create-template.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abyPZKWGixTg"
      },
      "source": [
        "# Step 3: Use Dashboard Features to Compare Models\n",
        "Here, we see that the two models are very similar in general distribution and prediction volumes. Prediction counts are similar, and distribution aren't significantly different either.\n",
        "\n",
        "**Let's dig deeper into soe accuracy statistics we are interested in.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrhmj6cHbuQ2"
      },
      "source": [
        "## Step 3.1 Monitoring Overall Accuracy Statistic\n",
        "Let's try creating some statistics widgets. We will first sets up a dashboard to **monitor our accuracy live in production**.\n",
        "\n",
        "Repeat for **Both Model A and B**:\n",
        "1. Use the create statistics widget\n",
        "2. Edit the statistics widget\n",
        "3. Select the model we logged, and **Evaluation Metrics**\n",
        "4. Select **Production** as **Model Environment**\n",
        "5. Finally, select **Accuracy** as your metric.\n",
        "\n",
        "\n",
        "**✏️ You can click on the gifs to replay it**\n",
        "[![image.png](https://storage.googleapis.com/arize-assets/fixtures/compare-accuracy.gif)](https://storage.googleapis.com/arize-assets/fixtures/compare-accuracy.gif)\n",
        "\n",
        "Now, we see that Model A and B accuracies are 0.451 and 0.454 respectively. Not very informative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZj6_o-3o3Gz"
      },
      "source": [
        "## Step 3.2 Using Filter in Widgets\n",
        "It seems like we can't tell much from accuracy alone. Let's user Arize to investigate our model performance in **certain important segments of our user**, in accordance to our business objective to be moore fair to those with `fico_score <= 500` (as stated in the beginning of this tutorial!)\n",
        "\n",
        "Repeat for **Both Model A and B**:\n",
        "1. Use the create statistics widget\n",
        "2. Follow step 2-5 the same way as earlier step.\n",
        "3. At the bottom, under **Filter**, Filter over **Feature** with **`fico_score`** that is **`<=`** to value of `500`.\n",
        "\n",
        "**✏️ You can click on the gifs to replay it**\n",
        "[![image.png](https://storage.googleapis.com/arize-assets/fixtures/compare-accuracy-low-fico.gif)](https://storage.googleapis.com/arize-assets/fixtures/compare-accuracy-low-fico.gif)\n",
        "\n",
        "It seems like our **Model A** is indeed better in this segment of our user. Let's investigate a little more with Arize features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFVurH5zb2YM"
      },
      "source": [
        "## Step 3.3 Set up Feature Distribution with Heatmap\n",
        "Now that we know there is something up, let's use the **Distribution Widget** feature to observe why accuracy is so low in this segment of users.\n",
        "\n",
        "Repeat for **Both Model A and Model B**\n",
        "1. Use the create distribution widget\n",
        "2. Set **Title** and **Plot 1 Title** as anything you want\n",
        "3. Select the model and **Model Environment** as **Production**\n",
        "4. Select **Distribution over** as **Feature** and **`fico_score`**\n",
        "5. **_Heatmap_**: Select **Distribution of** and **Accuracy**\n",
        "\n",
        "**Note:** After step 5, you should see that our data distribution has three distinct jumps in total proportion (under 500, 500-600, and above 600). For better frame reference in our heatmap, let's _filter under 600_ for a better view.\n",
        "\n",
        "6. For the **_Filter_** option, Select **Feature**, **`fico_score`**, **`<=`**, and **600**\n",
        "\n",
        "**✏️ You can click on the gifs to replay it**\n",
        "[![image.png](https://storage.googleapis.com/arize-assets/fixtures/compare-heatmap-low-fico.gif)](https://storage.googleapis.com/arize-assets/fixtures/compare-heatmap-low-fico.gif)\n",
        "\n",
        "From the two distributions of **Model A** and **Model B**, we can see that when `fico_score` steps under 500 for **Model B**, the model performance sharply declines. For this reason, we should pick model A even if the accuracy is a few percent worse -- since we value this segment of our customer for fairness reasons!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygXnRrVvtNZW"
      },
      "source": [
        "# ✏️ Takeaways\n",
        "In this toy example, **we knew we cared about this segment of users ahead of time,** and one model happened to be better at predicting this segment of users.\n",
        "\n",
        "![image.png](https://storage.googleapis.com/arize-assets/fixtures/compare-model-heatmap.png)\n",
        "\n",
        "Using the **heatmap** feature showcases in **Step 3.3**, we can actually explore in which segment our users (or features) are we underperforming, and use that observation as a basis for analyzing which model do we want to use in production.\n",
        "\n",
        "### 🚀 In short: Arize allows DS and ML teams to do more than **_Monitoring_** model statistics, but allows you to deep dive into truly **_Understanding_** your model in production and better serve their goals.\n",
        "\n",
        "Equiped the **Model Performance Dashboard** feature, we can create many continuous monitoring widgets so that we can always ship the most value out of our ML models in production.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QnAbdb7VV6Zi"
      },
      "source": [
        "### Overview\n",
        "Arize is an end-to-end ML observability and model monitoring platform. The platform is designed to help ML engineers and data science practitioners surface and fix issues with ML models in production faster with:\n",
        "- Automated ML monitoring and model monitoring\n",
        "- Workflows to troubleshoot model performance\n",
        "- Real-time visualizations for model performance monitoring, data quality monitoring, and drift monitoring\n",
        "- Model prediction cohort analysis\n",
        "- Pre-deployment model validation\n",
        "- Integrated model explainability\n",
        "\n",
        "### Website\n",
        "Visit Us At: https://arize.com/model-monitoring/\n",
        "\n",
        "### Additional Resources\n",
        "- [What is ML observability?](https://arize.com/what-is-ml-observability/)\n",
        "- [Playbook to model monitoring in production](https://arize.com/the-playbook-to-monitor-your-models-performance-in-production/)\n",
        "- [Using statistical distance metrics for ML monitoring and observability](https://arize.com/using-statistical-distance-metrics-for-machine-learning-observability/)\n",
        "- [ML infrastructure tools for data preparation](https://arize.com/ml-infrastructure-tools-for-data-preparation/)\n",
        "- [ML infrastructure tools for model building](https://arize.com/ml-infrastructure-tools-for-model-building/)\n",
        "- [ML infrastructure tools for production](https://arize.com/ml-infrastructure-tools-for-production-part-1/)\n",
        "- [ML infrastructure tools for model deployment and model serving](https://arize.com/ml-infrastructure-tools-for-production-part-2-model-deployment-and-serving/)\n",
        "- [ML infrastructure tools for ML monitoring and observability](https://arize.com/ml-infrastructure-tools-ml-observability/)\n",
        "\n",
        "Visit the [Arize Blog](https://arize.com/blog) and [Resource Center](https://arize.com/resource-hub/) for more resources on ML observability and model monitoring.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "[V2] Comparing Models A and B.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
